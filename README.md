# PicoGPT
PicoGPT is an unnecessarily tiny and minimal implementation of [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) in plain [NumPy](https://numpy.org). The entire forward pass code is [40 lines of code](https://github.com/jaymody/picoGPT/blob/main/gpt2_pico.py#L3-L41).

> This project is forked from [jaymody/picoGPT](https://github.com/jaymody/picoGPT). 
>
> The article content is translated and edited from  [GPT in 60 Lines of Numpy](https://jaykmody.com/blog/gpt-from-scratch/).

A quick breakdown of each of the files:

* `encoder.py` contains the code for OpenAI's BPE Tokenizer, taken straight from their [gpt-2 repo](https://github.com/openai/gpt-2/blob/master/src/encoder.py).
* `utils.py` contains the code to download and load the GPT-2 model weights, tokenizer, and hyper-parameters.
* `gpt2.py` contains the actual GPT model and generation code which we can run as a python script.
* `gpt2_pico.py` is the same as `gpt2.py`, but in even fewer lines of code. Why? Because why not ğŸ˜ğŸ‘.

## Dependencies

```bash
pip install -r requirements.txt
```
Tested on `Python 3.9.10`.

## Usage

```bash
python gpt2.py "Alan Turing theorized that computers would one day become"
```

Which generates

```
 the most powerful machines on the planet.

The computer is a machine that can perform complex calculations, and it can perform these calculations in a way that is very similar to the human brain.
```

You can also control the number of tokens to generate, the model size (one of `["124M", "355M", "774M", "1558M"]`), and the directory to save the models:

```bash
python gpt2.py \
    "Alan Turing theorized that computers would one day become" \
    --n_tokens_to_generate 40 \
    --model_size "124M" \
    --models_dir "models"
```

---

# 60 è¡Œ NumPy å®ç° GPT

# å‰è¨€

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ç”¨ 60 è¡Œä»£ç å®ç°ä¸€ä¸ª GPTï¼Œç»“åˆ OpenAI å‘å¸ƒçš„ç»è¿‡è®­ç»ƒçš„ GPT-2 æ¨¡å‹æƒï¼Œç”Ÿæˆä¸€äº›æ–‡æœ¬ã€‚

- æœ¬æ–‡å‡å®šè¯»è€…ç†Ÿæ‚‰ Pythonï¼ŒNumpyï¼Œè¿˜æœ‰ä¸€äº›è®­ç»ƒç¥ç»ç½‘ç»œçš„åŸºæœ¬ç»éªŒã€‚
- æ­¤å®ç°æ˜¯ä»¥æ•™è‚²ä¸ºç›®çš„ï¼Œå®ƒæ•…æ„ç¼ºå°‘è®¸å¤šåŠŸèƒ½ï¼Œä»¥å°½å¯èƒ½ç®€å•çš„åŒæ—¶ä¿æŒå®Œæ•´æ€§ã€‚

# GPT æ˜¯ä»€ä¹ˆ?

**GPT(Generative Pre-trained Transformer)**ï¼Œæ˜¯ä¸€ç±»åŸºäº Transformer çš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚[How GPT3 Works - Visualizations and Animations](https://jalammar.github.io/how-gpt3-works-visualizations-animations/) å¯¹ GPT è¿›è¡Œäº†ä»‹ç»ï¼š

- **ç”Ÿæˆå¼(Generative)ï¼š**GPT å¯ä»¥ç”Ÿæˆæ–‡æœ¬ï¼›
- **é¢„è®­ç»ƒ(Pre-trained)ï¼š**GPT åŸºäºæ¥è‡ªäºä¹¦æœ¬ã€äº’è”ç½‘ç­‰çš„æµ·é‡æ–‡æœ¬è¿›è¡Œè®­ç»ƒï¼›
- **Transformerï¼š**GPTæ˜¯ä¸€ä¸ªåªç”¨â€œè§£ç å™¨â€(decoder-only)çš„ Transformer ç¥ç»ç½‘ç»œç»“æ„ã€‚

> Transformer åŸæœ¬æœ‰â€œç¼–ç å™¨â€å’Œâ€œè§£ç å™¨â€ä¸¤éƒ¨åˆ†ï¼Œç¼–ç å™¨è´Ÿè´£ç†è§£è¾“å…¥ï¼Œè§£ç å™¨è´Ÿè´£ç”Ÿæˆè¾“å‡ºã€‚GPT åªä¿ç•™äº†è§£ç å™¨éƒ¨åˆ†ï¼Œæ‰€ä»¥å«â€œdecoder-onlyâ€ã€‚

åƒ OpenAI çš„ GPT-3 è¿™æ ·çš„å¤§å‹è¯­è¨€æ¨¡å‹ (LLM, Large Language Models)çš„çš„åº•å±‚éƒ½æ˜¯ GPTã€‚å®ƒä»¬çš„ç‰¹æ®Šä¹‹å¤„åœ¨äºï¼šç»è¿‡å¤§é‡æ•°æ®çš„è®­ç»ƒã€è§„æ¨¡éå¸¸å¤§ï¼š

1. [OpenAI GPT-3](https://huggingface.co/papers/2005.14165)ï¼šå‚æ•°é‡çº¦ 1750 äº¿(175B)ï¼Œè®­ç»ƒæ•°æ®çº¦ 3000 äº¿ä¸ª tokenï¼Œæ•°æ®é‡ 45TB å·¦å³ï¼›
2. [Google LaMDA](https://huggingface.co/papers/2201.08239)ï¼šLaMDA 1 ä»£çº¦ 1370 äº¿(137B)ï¼Œåœ¨é¢„è®­ç»ƒé˜¶æ®µæ”¶é›†å¹¶åˆ›å»ºäº†ä¸€ä¸ªå…·æœ‰ 1.56T å•è¯çš„æ•°æ®é›†ã€‚

## è¾“å…¥å’Œè¾“å…¥

ä¸€ä¸ª GPT çš„å‡½æ•°ç­¾åç±»ä¼¼è¿™æ ·ï¼š

```python
def gpt(inputs: list[int]) -> list[list[float]]:
    #
    # inputs çš„å½¢çŠ¶æ˜¯ [n_seq]
    # [n_seq] è¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªä¸€ç»´æ•°ç»„ï¼Œé•¿åº¦ä¸º n_seq
    # åœ¨ NLP é‡Œï¼Œé€šå¸¸è¡¨ç¤ºä¸€ä¸ªåºåˆ—(æ¯”å¦‚ä¸€å¥è¯é‡Œæœ‰å¤šå°‘ä¸ª token)
    #
    # output çš„å½¢çŠ¶æ˜¯ [n_seq, n_vocab]
    # è¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œæœ‰ n_seq è¡Œã€ n_vocab åˆ—
    # åœ¨ NLP é‡Œï¼Œé€šå¸¸è¡¨ç¤ºæ¯ä¸ª token çš„è¾“å‡ºæ˜¯ä¸€ä¸ªé•¿åº¦ä¸º n_vocab çš„å‘é‡ï¼Œæ¯”å¦‚æ¯ä¸ª token é¢„æµ‹è¯è¡¨ä¸­æ¯ä¸ªè¯çš„æ¦‚ç‡
    #
    output = # ç¥ç»ç½‘ç»œçš„ç¥å¥‡é­”æ³•
    return output
```

è¾“å…¥æ˜¯ä¸€äº›æ–‡æœ¬ï¼Œè¿™äº›æ–‡æœ¬è¢«è¡¨ç¤ºæˆä¸€ä¸²æ•´æ•°åºåˆ—ï¼Œæ¯ä¸ªæ•´æ•°éƒ½ä¸æ–‡æœ¬ä¸­çš„ token å¯¹åº”ï¼š

```python
# æ•´æ•°è¡¨ç¤ºæ–‡æœ¬ä¸­çš„ tokenï¼Œä¾‹å¦‚ï¼š
# text  = "not all heroes wear capes"
# token = "not" "all" "heroes" "wear" "capes"
inputs =   [1,     0,       2,     4,      6]
```

token æ˜¯æ–‡æœ¬çš„å°ç‰‡æ®µï¼Œå®ƒä»¬ç”±æŸç§**åˆ†è¯å™¨(Tokenizer)**äº§ç”Ÿã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€ä¸ª**è¯è¡¨(Vocabulary)**å°† token æ˜ å°„ä¸ºæ•´æ•°ï¼š

```python
# è¯è¡¨ä¸­æ¯ä¸ª token çš„ index å°±æ˜¯è¯¥ token çš„æ•´æ•° id
# ä¾‹å¦‚ "heroes" çš„ index æ˜¯ 2ï¼Œå› ä¸º vocab[2] = "heroes"
vocab = ["all", "not", "heroes", "the", "wear", ".", "capes"]

# ä¸€ä¸ªå‡æƒ³çš„åˆ†è¯å™¨ï¼ŒæŒ‰ç©ºæ ¼è¿›è¡Œåˆ†è¯
tokenizer = WhitespaceTokenizer(vocab)

# encode() æ–¹æ³•å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°åˆ—è¡¨
ids = tokenizer.encode("not all heroes wear") # ids = [1, 0, 2, 4]

# å¯ä»¥é€šè¿‡è¯è¡¨æ˜ å°„æŸ¥çœ‹å®é™…çš„ token
tokens = [tokenizer.vocab[i] for i in ids] # tokens = ["not", "all", "heroes", "wear"]

# decode() æ–¹æ³•å°†æ•´æ•°åˆ—è¡¨è½¬æ¢å›å­—ç¬¦ä¸²
text = tokenizer.decode(ids) # text = "not all heroes wear"
```

ç®€å•è¯´ï¼šæˆ‘ä»¬æœ‰ä¸€ä¸ªå­—ç¬¦ä¸²ã€æˆ‘ä»¬ä½¿ç”¨åˆ†è¯å™¨å°†å…¶æ‹†è§£ä¸ºå°ç‰‡æ®µâ€”â€” tokenã€æˆ‘ä»¬ä½¿ç”¨è¯æ±‡è¡¨å°†è¿™äº› token æ˜ å°„ä¸ºæ•´æ•°ã€‚

åœ¨å®é™…ä¸­ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨ä¸€äº›æ›´é«˜çº§çš„åˆ†è¯å™¨ï¼Œå¦‚ [Byte-Pair Encoding](https://huggingface.co/learn/llm-course/chapter6/5) æˆ–è€… [WordPiece](https://huggingface.co/learn/llm-course/chapter6/6?fw=pt) ç­‰ï¼Œå…¶åŸç†æ˜¯ä¸€è‡´çš„ï¼š

1. æœ‰ä¸€ä¸ª `vocab` å°†å­—ç¬¦ä¸²æ ‡è®°æ˜ å°„åˆ°æ•´æ•°ç´¢å¼•ï¼›
2. æœ‰ä¸€ç§ `encode` æ–¹æ³•å¯ä»¥è½¬æ¢ `str -> list[int]`ï¼›
3. æœ‰ä¸€ç§ `decode` æ–¹æ³•å¯ä»¥è½¬æ¢ `list[int] -> str`ã€‚

è¾“å‡ºæ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œå…¶ä¸­ `output[i][j` è¡¨ç¤ºæ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡ï¼Œè¿™ä¸ªæ¦‚ç‡ä»£è¡¨äº†è¯æ±‡è¡¨ä¸­ä½äº `vocab[j]` çš„ token æ˜¯ä¸‹ä¸€ä¸ª token `inputs[i+1]` çš„æ¦‚ç‡ï¼Œå¦‚ï¼š

```python
vocab = ["all", "not", "heroes", "the", "wear", ".", "capes"]
inputs = [1, 0, 2, 4] # "not" "all" "heroes" "wear"
output = gpt(inputs)

#              ["all", "not", "heroes", "the", "wear", ".", "capes"]
# output[0] =  [0.75    0.1       0.0   0.15     0.0  0.0      0.0 ]
# åªç»™å®š "not"ï¼Œæ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ˜¯ "all" çš„æ¦‚ç‡æœ€å¤§

#              ["all", "not", "heroes", "the", "wear", ".", "capes"]
# output[1] =  [ 0.0    0.0       0.8    0.1     0.0   0.0     0.1 ]
# ç»™å®šåºåˆ— ["not", "all"]ï¼Œæ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ˜¯ "heroes" çš„æ¦‚ç‡æœ€å¤§

#              ["all", "not", "heroes", "the", "wear", ".", "capes"]
# output[-1] = [ 0.0    0.0       0.0    0.1     0.0 0.05     0.85 ]
# ç»™å®šæ•´ä¸ªåºåˆ— ["not", "all", "heroes", "wear"]ï¼Œæ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ˜¯ "capes" çš„æ¦‚ç‡æœ€å¤§
```

ä¸ºäº†è·å¾—æ•´ä¸ªåºåˆ—çš„ä¸‹ä¸€ä¸ª token é¢„æµ‹ï¼Œæˆ‘ä»¬åªéœ€å–æ¦‚ç‡æœ€é«˜çš„æ ‡è®°  `output[-1]`ï¼š

```python
vocab = ["all", "not", "heroes", "the", "wear", ".", "capes"]
inputs = [1, 0, 2, 4] # "not" "all" "heroes" "wear"
output = gpt(inputs)
next_token_id = np.argmax(output[-1]) # next_token_id = 6
next_token = vocab[next_token_id]     # next_token = "capes"
```

å°†å…·æœ‰æœ€é«˜æ¦‚ç‡çš„ token ä½œä¸ºç»“æœï¼Œå«åš**[è´ªå©ªè§£ç (Greedy decoding)](https://docs.cohere.com/docs/controlling-generation-with-top-k-top-p#1-pick-the-top-token-greedy-decoding)**æˆ–è€…**è´ªå©ªé‡‡æ ·(Greedy sampling)**ã€‚

é¢„æµ‹åºåˆ—ä¸­ä¸‹ä¸€ä¸ªæœ€åˆç†å•è¯çš„ä»»åŠ¡è¢«ç§°ä¸º**è¯­è¨€å»ºæ¨¡(Language modeling)**ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠ GPT ç§°ä¸ºä¸€ç§**è¯­è¨€æ¨¡å‹(Language model)**ã€‚

## ç”Ÿæˆæ–‡æœ¬

### è‡ªå›å½’(Autoregressive)

æˆ‘ä»¬å¯ä»¥è¿­ä»£åœ°ä»æ¨¡å‹ä¸­è·å–ä¸‹ä¸€ä¸ª token æ¥ç”Ÿæˆå®Œæ•´çš„å¥å­ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬å°†é¢„æµ‹çš„ token æ·»åŠ å›è¾“å…¥ï¼š

```python
def generate(inputs, n_tokens_to_generate):
    for _ in range(n_tokens_to_generate): # è‡ªå›å½’ decode å¾ªç¯
        output = gpt(inputs)              # æ¨¡å‹æ¨ç†
        next_id = np.argmax(output[-1])   # è´ªå©ªé‡‡æ ·
        inputs.append(int(next_id))       # å°†é¢„æµ‹ç»“æœæ·»åŠ å›è¾“å…¥
    return inputs[len(inputs) - n_tokens_to_generate :]  # åªè¿”å›ç”Ÿæˆçš„ token id

input_ids = [1, 0]                             # "not" "all"
output_ids = generate(input_ids, 3)            # output_ids = [2, 4, 6]
output_tokens = [vocab[i] for i in output_ids] # "heroes" "wear" "capes"
```

é¢„æµ‹æœªæ¥å€¼(Regression)å¹¶å°†å…¶æ·»åŠ å›è¾“å…¥(auto)çš„è¿‡ç¨‹å°±æ˜¯ GPT è¢«æè¿°ä¸º**è‡ªå›å½’(Autoregressive)**çš„åŸå› ã€‚

### é‡‡æ ·(Sampling)

æˆ‘ä»¬å¯ä»¥é€šè¿‡æŒ‰ç…§æ¦‚ç‡åˆ†å¸ƒéšæœºé€‰æ‹©ä¸€ä¸ª token æ¥å¼•å…¥ä¸€äº›**éšæœºæ€§(Stochasticity)ï¼š**

```python
inputs = [1, 0, 2, 4] # "not" "all" "heroes" "wear"
output = gpt(inputs)
np.random.choice(np.arange(vocab_size), p=output[-1]) # capes
np.random.choice(np.arange(vocab_size), p=output[-1]) # hats
np.random.choice(np.arange(vocab_size), p=output[-1]) # capes
np.random.choice(np.arange(vocab_size), p=output[-1]) # capes
np.random.choice(np.arange(vocab_size), p=output[-1]) # pants
```

è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿé’ˆå¯¹ç›¸åŒçš„è¾“å…¥ç”Ÿæˆä¸åŒçš„å¥å­ã€‚å½“ä¸ [**top-k**](https://docs.cohere.ai/docs/controlling-generation-with-top-k-top-p#2-pick-from-amongst-the-top-tokens-top-k)ã€[**top-p**](https://docs.cohere.ai/docs/controlling-generation-with-top-k-top-p#3-pick-from-amongst-the-top-tokens-whose-probabilities-add-up-to-15-top-p) å’Œ [**temperature **](https://docs.cohere.ai/docs/temperature)ç­‰åœ¨é‡‡æ ·å‰ä¿®æ”¹åˆ†å¸ƒçš„æŠ€æœ¯ç»“åˆä½¿ç”¨æ—¶ï¼Œæˆ‘ä»¬çš„è¾“å‡ºè´¨é‡å°†å¾—åˆ°æ˜¾è‘—æå‡ã€‚è¿™äº›æŠ€æœ¯è¿˜å¼•å…¥äº†ä¸€äº›**è¶…å‚æ•°(Hyperparameter)**ï¼Œè¿™äº›è¶…å‚æ•°ç”±å¼€å‘è€…åœ¨è®­ç»ƒå¼€å§‹å‰æ‰‹åŠ¨è®¾å®šçš„å‚æ•°ï¼Œè€Œä¸æ˜¯é€šè¿‡æ¨¡å‹è‡ªåŠ¨å­¦ä¹ å¾—åˆ°çš„å‚æ•°ã€‚æˆ‘ä»¬å¯ä»¥å°è¯•è°ƒæ•´è¿™äº›è¶…å‚æ•°æ¥è·å¾—ä¸åŒçš„ç”Ÿæˆè¡Œä¸ºã€‚

> 1. top-k ï¼šæ¯æ¬¡åªåœ¨æ¦‚ç‡æœ€é«˜çš„å‰ k ä¸ªè¯é‡Œéšæœºé€‰ä¸€ä¸ªï¼Œå¢åŠ å¤šæ ·æ€§ï¼Œé¿å…æ€»æ˜¯é€‰æ¦‚ç‡æœ€å¤§çš„è¯ã€‚
> 2. top-p ï¼šæ¯æ¬¡åªåœ¨ç´¯è®¡æ¦‚ç‡è¾¾åˆ° p çš„è¯é›†åˆé‡Œéšæœºé€‰ä¸€ä¸ªï¼ŒåŠ¨æ€æ§åˆ¶å€™é€‰è¯æ•°é‡ï¼Œè¿›ä¸€æ­¥æå‡ç”Ÿæˆçš„è‡ªç„¶åº¦å’Œå¤šæ ·æ€§ã€‚
> 3. temperature ï¼šé€šè¿‡è°ƒæ•´æ¦‚ç‡åˆ†å¸ƒçš„â€œå¹³æ»‘åº¦â€ï¼Œæ¸©åº¦é«˜æ—¶ç”Ÿæˆæ›´éšæœºï¼Œæ¸©åº¦ä½æ—¶ç”Ÿæˆæ›´ç¡®å®šã€‚

## è®­ç»ƒ

æˆ‘ä»¬ä¸è®­ç»ƒå…¶å®ƒç¥ç»ç½‘ç»œä¸€æ ·ï¼Œ**æŸå¤±å‡½æ•°(Loss Function)**æ˜¯è¡¡é‡æ¨¡å‹é¢„æµ‹ç»“æœä¸çœŸå®ç»“æœä¹‹é—´å·®è·çš„ä¸€ä¸ªå‡½æ•°ï¼Œé’ˆå¯¹ç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼Œä½¿ç”¨[æ¢¯åº¦ä¸‹é™(Gradient descent optimization algorithms)](https://huggingface.co/papers/1609.04747)è®­ç»ƒ GPTï¼Œå³æ ¹æ®æŸå¤±å‡½æ•°å¯¹å‚æ•°çš„å¯¼æ•°(æ¢¯åº¦)ï¼Œæ²¿ç€è®©æŸå¤±å˜å°çš„æ–¹å‘ï¼Œå¾®è°ƒæ¨¡å‹å‚æ•°ã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨**è¯­è¨€å»ºæ¨¡ä»»åŠ¡(Language Modeling)**å³ç»™å®šä¸€æ®µæ–‡æœ¬çš„å‰é¢éƒ¨åˆ†ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªæœ€æœ‰å¯èƒ½å‡ºç°çš„ token çš„ä»»åŠ¡ï¼Œçš„[**äº¤å‰ç†µæŸå¤±(Cross Entropy Loss)**](https://www.youtube.com/watch?v=ErfnhcEV1O8)æŸå¤±å‡½æ•°ï¼šå‡è®¾æœ‰ä¸‰ä¸ªç±»åˆ«ï¼ŒçœŸå®æ ‡ç­¾æ˜¯ç±»åˆ« 2ï¼Œæ¨¡å‹é¢„æµ‹æ¦‚ç‡ä¸º [0.1, 0.7, 0.2]ï¼Œé‚£ä¹ˆäº¤å‰ç†µæŸå¤±å°±æ˜¯ -log(0.7)ã€‚å¦‚æœæ¨¡å‹é¢„æµ‹æ¦‚ç‡ä¸º [0.1, 0.2, 0.7]ï¼ŒæŸå¤±å°±æ˜¯ -log(0.2)ï¼Œæ˜¾ç„¶æŸå¤±æ›´å¤§ï¼Œå› ä¸ºæ¨¡å‹é¢„æµ‹é”™äº†ã€‚å…·ä½“å¯è§ä»£ç ï¼š

```python
def lm_loss(inputs: list[int], params) -> float:
    # æ ‡ç­¾ y åªæ˜¯ inputs å‘å·¦ç§»åŠ¨ 1 ä½
    # 
    # inputs = [not, all, heroes, wear, capes]
    #      x = [not, all, heroes, wear]
    #      y =      [all, heroes, wear, capes]
    # 
    # å¯¹äºé•¿åº¦ä¸º N çš„ inputsï¼Œæˆ‘ä»¬å¯ä»¥æ„å»º N-1 ä¸ª"è¾“å…¥-æ ‡ç­¾"å¯¹
    # ä¸¤è€…çš„å½¢çŠ¶éƒ½æ˜¯ [åºåˆ—ä¸­çš„ token æ•° - 1]
    x, y = inputs[:-1], inputs[1:] 
    
    # å‰å‘ä¼ æ’­(Forward pass): æŠŠè¾“å…¥æ•°æ®ä¾æ¬¡é€šè¿‡ç¥ç»ç½‘ç»œçš„æ¯ä¸€å±‚ï¼Œæœ€ç»ˆå¾—åˆ°è¾“å‡ºã€‚
    #
    # åœ¨æ¯ä¸ªä½ç½®é¢„æµ‹çš„ä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡åˆ†å¸ƒ
    output = gpt(x, params) # å½¢çŠ¶ä¸º [åºåˆ—ä¸­çš„ token æ•° - 1, è¯è¡¨ä¸­çš„ token æ•°]
    
    # äº¤å‰ç†µæŸå¤±
    # æˆ‘ä»¬å¯¹æ‰€æœ‰ N-1 ä¸ªç¤ºä¾‹å–å¹³å‡å€¼
    # å¯¹äºäºŒç»´æ•°ç»„ output ï¼Œåœ¨ç¬¬ i è¡Œï¼Œå–ç¬¬ y[i] åˆ—çš„å…ƒç´ 
    # æœ€ç»ˆå¾—åˆ°çš„æ˜¯ä¸€ä¸ªä¸€ç»´æ•°ç»„ï¼Œé•¿åº¦ç­‰äºè¡Œæ•°ï¼Œæ¯ä¸ªå…ƒç´ å°±æ˜¯æ¨¡å‹åœ¨è¯¥ä½ç½®é¢„æµ‹çœŸå®ä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡
    loss = np.mean(-np.log(output[np.arange(len(output)), y]))
    
    return loss

def train(texts: list[list[str]], params) -> float:
    for text in texts:
      	#
      	# ç”¨ tokenizer.encode(text) æŠŠæ–‡æœ¬è½¬æˆ token id çš„åºåˆ—ï¼Œæ–¹ä¾¿æ¨¡å‹å¤„ç†
        inputs = tokenizer.encode(text)
        #
        # è®¡ç®—å½“å‰æ¨¡å‹åœ¨è¿™æ¡æ•°æ®ä¸Šçš„æŸå¤±ï¼Œè¡¡é‡æ¨¡å‹é¢„æµ‹å’ŒçœŸå®ç­”æ¡ˆçš„å·®è·
        loss = lm_loss(inputs, params)
        #
        # åå‘ä¼ æ’­ç®—æ³•(backpropagation): æ ¹æ®æŸå¤±å‡½æ•°çš„ç»“æœï¼Œè‡ªåŠ¨è®¡ç®—æ¯ä¸ªå‚æ•°å¯¹æŸå¤±çš„å½±å“,ç”¨äºæŒ‡å¯¼å‚æ•°å¦‚ä½•è°ƒæ•´
        #
        # é€šè¿‡åå‘ä¼ æ’­ç®—æ³•è®¡ç®—æŸå¤±å¯¹å‚æ•°çš„æ¢¯åº¦ï¼Œå³æ¯ä¸ªå‚æ•°è¯¥æ€ä¹ˆè°ƒæ•´èƒ½è®©æŸå¤±å˜å°
        gradients = compute_gradients_via_backpropagation(loss, params)
        #
        # æ ¹æ®æ¢¯åº¦è°ƒæ•´å‚æ•°ï¼Œå³â€œä¼˜åŒ–â€æ¨¡å‹ï¼Œè®©æ¨¡å‹è¡¨ç°æ›´å¥½
        params = gradient_descent_update_step(gradients, params)
    #
    # æ‰€æœ‰æ–‡æœ¬éƒ½è®­ç»ƒä¸€éåï¼Œè¿”å›æ›´æ–°åçš„å‚æ•°
    return params
```

è¿™å°±æ˜¯ä¸€ä¸ªæåº¦ç®€åŒ–ä½†å…¸å‹çš„ç¥ç»ç½‘ç»œè®­ç»ƒå¾ªç¯ï¼šç¼–ç è¾“å…¥ã€è®¡ç®—æŸå¤±ã€åå‘ä¼ æ’­æ±‚æ¢¯åº¦ã€ç”¨æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°å‚æ•°ã€‚ä¸æ–­é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œæ¨¡å‹å°±ä¼šè¶Šæ¥è¶Šâ€œèªæ˜â€ï¼Œé¢„æµ‹èƒ½åŠ›è¶Šæ¥è¶Šå¼ºã€‚

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå¹¶æœªä½¿ç”¨æ˜ç¡®çš„æ ‡æ³¨æ•°æ®ã€‚å–è€Œä»£ä¹‹çš„æ˜¯ä½¿ç”¨åŸå§‹æ–‡æœ¬è‡ªèº«ï¼Œäº§ç”Ÿå¤§é‡çš„è¾“å…¥/æ ‡ç­¾å¯¹(input/label pairs)ã€‚è¿™å°±æ˜¯æ‰€è°“çš„**[è‡ªç›‘ç£å­¦ä¹ (Self-supervised learning)](https://en.wikipedia.org/wiki/Self-supervised_learning)**ã€‚è‡ªç›‘ç£ä½¿æˆ‘ä»¬èƒ½å¤Ÿå¤§è§„æ¨¡æ‰©å±•è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬åªéœ€è¦è·å–å°½å¯èƒ½å¤šçš„åŸå§‹æ–‡æœ¬ï¼Œå¹¶å°†å…¶è¾“å…¥åˆ°æ¨¡å‹ä¸­å³å¯ã€‚ä¾‹å¦‚ï¼ŒGPT-3 ä½¿ç”¨äº†æ¥è‡ªäº’è”ç½‘å’Œä¹¦ç±çš„ 3000 äº¿ä¸ªæ–‡æœ¬ tokens è¿›è¡Œè®­ç»ƒï¼š

![Datasets used to train GPT-3](./README.assets/datasets_used_to_train_gpt_3.png)

è¿™ä¸ªè‡ªç›‘ç£è®­ç»ƒçš„æ­¥éª¤ç§°ä¹‹ä¸º**é¢„è®­ç»ƒ(Pre-training)**ï¼Œæˆ‘ä»¬å¯ä»¥é‡å¤ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æƒé‡æ¥è®­ç»ƒä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„ç‰¹å®šæ¨¡å‹ï¼Œé¢„è®­ç»ƒæ¨¡å‹æœ‰æ—¶ä¹Ÿè¢«ç§°ä¸º**åŸºç¡€æ¨¡å‹(Foundation models)**ã€‚åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè®­ç»ƒæ¨¡å‹è¢«ç§°ä¹‹ä¸º**å¾®è°ƒ(Fine-tuning)**ï¼Œå› ä¸ºæ¨¡å‹æƒé‡å·²ç»è¿‡é¢„å…ˆè®­ç»ƒä»¥ç†è§£è¯­è¨€ï¼Œå› æ­¤å®ƒåªæ˜¯é’ˆå¯¹æ‰‹å¤´çš„ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚

è¿™ç§â€œåœ¨é€šç”¨ä»»åŠ¡ä¸Šé¢„è®­ç»ƒ+åœ¨ç‰¹å®šä»»åŠ¡ä¸Šå¾®è°ƒâ€çš„ç­–ç•¥ï¼Œç§°ä¹‹ä¸º**[è¿ç§»å­¦ä¹ (Transfer learning)](https://en.wikipedia.org/wiki/Transfer_learning)**ã€‚

## æç¤º(Prompting)

æœ€åˆçš„ [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) ä»‹ç»äº†ä¸€ç§ä½¿ç”¨ Transformer æ¶æ„çš„ç”Ÿæˆå¼é¢„è®­ç»ƒæ¨¡å‹ï¼Œæå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼šé¦–å…ˆåœ¨å¤§é‡æ— æ ‡æ³¨æ–‡æœ¬ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€ç„¶ååœ¨æœ‰æ ‡æ³¨æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚ä¸€ä¸ª 117M å‚æ•°è§„æ¨¡çš„ GPT é¢„è®­ç»ƒæ¨¡å‹ï¼Œåœ¨é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡çš„æ ‡æ³¨æ•°æ®ä¸Šå¾®è°ƒä¹‹åï¼Œå®ƒèƒ½å¤Ÿåœ¨å„ç§**è‡ªç„¶è¯­è¨€å¤„ç†(NLP, Natural language processing)**ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€ä¼˜æ€§èƒ½ã€‚

ç›´åˆ° [Language Models are Unsupervised Multitask Learners ](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)å’Œ [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) è®ºæ–‡å‘è¡¨ï¼Œæˆ‘ä»¬æ„è¯†åˆ°ï¼Œä¸€ä¸ªç”¨è¶³å¤Ÿå¤šçš„æ•°æ®å’Œå‚æ•°é¢„è®­ç»ƒçš„ GPT æ¨¡å‹ï¼Œèƒ½å¤Ÿç‹¬ç«‹æ‰§è¡Œä»»æ„ä»»åŠ¡ï¼Œæ— éœ€å¾®è°ƒã€‚åªè¦å¯¹æ¨¡å‹è¿›è¡Œ**æç¤º(Prompt)**ï¼Œæ¨¡å‹å°±ä¼šç»™å‡ºåˆé€‚çš„å“åº”ã€‚è¿™è¢«ç§°ä¸º**ä¸Šä¸‹æ–‡å­¦ä¹ (In-context learning)**ï¼Œå› ä¸ºæ¨¡å‹ä»…ä½¿ç”¨æç¤ºçš„ä¸Šä¸‹æ–‡æ¥æ‰§è¡Œä»»åŠ¡ã€‚ä¸Šä¸‹æ–‡å­¦ä¹ å¯ä»¥æ˜¯**é›¶æ ·æœ¬å­¦ä¹ (Zero-shot)**ã€**å•æ ·æœ¬å­¦ä¹ (One-shot)**æˆ–**å°‘æ ·æœ¬å­¦ä¹ (Few-shot)**ï¼š

![Language Models are Few-Shot Learners Figure 2.1: Zero-shot, one-shot and few-shot, contrasted with traditional fine-tuning](./README.assets/zero_shot_one_shot_and_few_shot_contrasted_with_traditional_fine_tuning.png)

åŸºäºæç¤ºå†…å®¹ç”Ÿæˆæ–‡æœ¬ä¹Ÿè¢«ç§°ä¹‹ä¸º**æ¡ä»¶ç”Ÿæˆ(Conditional generation)**ï¼Œå› ä¸ºæˆ‘ä»¬çš„æ¨¡å‹æ˜¯åŸºäºç‰¹å®šçš„è¾“å…¥(æ¡ä»¶)è¿›è¡Œå†…å®¹ç”Ÿæˆçš„ã€‚

GPT ä¸ä»…é™äºè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡(NLP)ï¼Œå®ƒå¯ä»¥åº”ç”¨äºå„ç§æ¡ä»¶ä¸‹çš„åœºæ™¯ã€‚ä½ å¯ä»¥å°†æ¨¡å‹ç”¨äºä»»ä½•ä½ æƒ³è¦çš„æ¡ä»¶ä¸‹ã€‚ä½ å¯ä»¥å°† GPT å˜æˆä¸€ä¸ªèŠå¤©æœºå™¨äººï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹çš„æ¡ä»¶å°±æ˜¯ä½ çš„å¯¹è¯å†å²ã€‚æ¨¡å‹ä¼šæ ¹æ®ä¹‹å‰çš„å¯¹è¯å†…å®¹æ¥ç”Ÿæˆåˆé€‚çš„å›å¤ã€‚é€šè¿‡åˆç†è®¾è®¡æç¤ºè¯ï¼Œå¯ä»¥åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ§åˆ¶æ¨¡å‹çš„è¾“å‡ºè¡Œä¸ºï¼Œä½†åŒæ—¶ä¹Ÿéœ€è¦æ³¨æ„å…¶æ½œåœ¨çš„å±€é™æ€§ã€‚

# å®ç° GPT

## ä¸‹è½½å’Œäº†è§£é¡¹ç›®

å…‹éš†æœ¬æ•™ç¨‹çš„å­˜å‚¨åº“ï¼Œå¹¶å®‰è£…ä¾èµ–é¡¹ï¼š

```
git clone https://github.com/jaymody/picoGPT
cd picoGPT
pip install -r requirements.txt
```

é¡¹ç›®æ–‡ä»¶åŒ…å«ï¼š

- **`encoder.py`** åŒ…å« OpenAI çš„ BPE åˆ†è¯å™¨(Tokenizer)çš„ä»£ç ï¼Œæ¥è‡ª [openai gpt-2](https://github.com/openai/gpt-2/blob/master/src/encoder.py)ï¼›
- **`utils.py`** åŒ…å«ä¸‹è½½å’ŒåŠ è½½ GPT-2 æ¨¡å‹çš„æƒé‡ã€åˆ†è¯å™¨å’Œè¶…å‚æ•°çš„ä»£ç ï¼›
- **`gpt2.py`** åŒ…å«å®é™…çš„ GPT æ¨¡å‹çš„ä»£ç åŠå®Œæ•´çš„æ³¨é‡Šæè¿°ï¼Œå¯ç›´æ¥è¿è¡Œï¼›
- **`gpt2_pico.py `**ä¸ `gpt2.py` ç›¸åŒï¼Œä½†çœå»äº†æ³¨é‡Šéƒ¨åˆ†ã€‚

é¦–å…ˆ

```python
import numpy as np

def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):
    pass # TODO: å¾…å®ç°

def generate(inputs, params, n_head, n_tokens_to_generate):
    from tqdm import tqdm
    # è‡ªå›å½’è§£ç å¾ªç¯
    for _ in tqdm(range(n_tokens_to_generate), "generating"): 
        logits = gpt2(inputs, **params, n_head=n_head)  # æ¨¡å‹å‰å‘ä¼ æ’­
        next_id = np.argmax(logits[-1])                 # è´ªå©ªé‡‡æ ·
        inputs.append(int(next_id))                     # å°†é¢„æµ‹ç»“æœæ·»åŠ åˆ°è¾“å…¥ä¸­

    return inputs[len(inputs) - n_tokens_to_generate :] # åªè¿”å›ç”Ÿæˆçš„ id

def main(prompt: str, n_tokens_to_generate: int = 40, model_size: str = "124M", models_dir: str = "models"):
    from utils import load_encoder_hparams_and_params
    # ä» openai gpt-2 æ–‡ä»¶ä¸­åŠ è½½ç¼–ç å™¨ã€è¶…å‚æ•°å’Œå‚æ•°
    encoder, hparams, params = load_encoder_hparams_and_params(model_size, models_dir)

    # ä½¿ç”¨ BPE åˆ†è¯å™¨å¯¹è¾“å…¥å­—ç¬¦ä¸²è¿›è¡Œç¼–ç 
    input_ids = encoder.encode(prompt)

    # ç¡®ä¿æˆ‘ä»¬ä¸è¶…è¿‡æ¨¡å‹çš„æœ€å¤§åºåˆ—é•¿åº¦
    assert len(input_ids) + n_tokens_to_generate < hparams["n_ctx"]

    # ç”Ÿæˆè¾“å‡º token
    output_ids = generate(input_ids, params, hparams["n_head"], n_tokens_to_generate)

    # å°† token è§£ç å›å­—ç¬¦ä¸²
    output_text = encoder.decode(output_ids)

    return output_text

if __name__ == "__main__":
    import fire
    fire.Fire(main)
```

ä¸Šè¿°ä»£ç åŒ…å«å››ä¸ªéƒ¨åˆ†ï¼š

1. `gpt2` å‡½æ•°æ˜¯æˆ‘ä»¬å°†è¦å®ç°çš„ GPTã€‚å‡½æ•°ç­¾åä¸­é™¤äº† `inputs`ï¼Œè¿˜æœ‰å…¶å®ƒçš„å‚æ•°ï¼š

   - `wte`ã€`wpe`ã€ `blocks`ã€ `ln_f` æ˜¯æ¨¡å‹çš„å‚æ•°ï¼›
   - `n_head` æ˜¯å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­éœ€è¦çš„è¶…å‚æ•°ï¼›

2. `generate` å‡½æ•°æ˜¯è‡ªå›å½’è§£ç ï¼Œä¸ºäº†ç®€æ´ï¼Œè¿™é‡Œå°†ä½¿ç”¨è´ªå©ªè§£ç ï¼›

   - [`tqdm`](https://github.com/tqdm/tqdm) æä¾›è¿›åº¦ï¼Œä»¥ç›´è§‚åœ°çœ‹åˆ°è§£ç è¿‡ç¨‹ï¼›

3. `main` å‡½æ•°ï¼š

   1. åŠ è½½åˆ†è¯å™¨(`encoder`)ã€æ¨¡å‹æƒé‡(`params`)ã€è¶…å‚æ•°(`hparams`)ï¼›

      ```python
      # è¿™å°†ä¸‹è½½å¿…è¦çš„æ–‡ä»¶åˆ° models/124M ä¸­
      from utils import load_encoder_hparams_and_params
      encoder, hparams, params = load_encoder_hparams_and_params("124M", "models")
      ```

      > åœ¨å¾ˆå¤šNLPåº“ä¸­ï¼Œ"encoder" è¿™ä¸ªæœ¯è¯­æœ‰æ—¶ä¼šç”¨æ¥æŒ‡ä»£åŒ…å« tokenization åŠŸèƒ½çš„å¯¹è±¡.
      >
      > åœ¨ç¤ºä¾‹çš„ä»£ç ä¸­ï¼Œencoder å¯¹è±¡å®é™…ä¸Šæ˜¯ä¸€ä¸ªtokenizerï¼Œå®ƒåŒ…å«:
      >
      > - encode æ–¹æ³•ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºtoken ID(tokenizeråŠŸèƒ½)ï¼›
      > - decoder å±æ€§ï¼šå°† token ID æ˜ å°„å›æ–‡æœ¬è¡¨ç¤ºã€‚

   2. ä½¿ç”¨ BPE åˆ†è¯å™¨å¯¹è¾“å…¥å­—ç¬¦ä¸²è¿›è¡Œç¼–ç ï¼›

   3. è°ƒç”¨ `generate` å‡½æ•°ç”Ÿæˆè¾“å‡º tokenï¼›

   4. å°† token è§£ç å›å­—ç¬¦ä¸²ä»¥è¾“å‡ºï¼›

4. [`fire.Fire(main)`](https://github.com/google/python-fire) è®©æ–‡ä»¶å˜æˆ CLI åº”ç”¨ç¨‹åºï¼Œæœ€ç»ˆå¯ä»¥ä½¿ç”¨å‘½ä»¤è¿è¡Œä»£ç ï¼š`python gpt2.py "some prompt here"`ã€‚

## åˆ†è¯å™¨ã€è¶…å‚æ•°ã€å‚æ•°

`encoder` æ˜¯ GPT-2 ä½¿ç”¨çš„ BPE åˆ†è¯å™¨ï¼š

```python
ids = encoder.encode("Not all heroes wear capes.")
ids
# [3673, 477, 10281, 5806, 1451, 274, 13]
```

ä½¿ç”¨åˆ†è¯å™¨çš„è¯æ±‡è¡¨ï¼Œå¯ä»¥æŸ¥çœ‹å®é™…çš„ tokenï¼š

```python
[encoder.decoder[i] for i in ids]
# ['Not', 'Ä all', 'Ä heroes', 'Ä wear', 'Ä cap', 'es', '.']
```

è¯·æ³¨æ„ï¼Œæœ‰æ—¶ token æ˜¯å•è¯(å¦‚ `Not`)ï¼Œæœ‰æ—¶æ˜¯å‰é¢æœ‰ä¸€ä¸ªç©ºæ ¼çš„å•è¯(å¦‚ `Ä all`ï¼Œ[`Ä `è¡¨ç¤ºç©ºæ ¼](https://github.com/karpathy/minGPT/blob/37baab71b9abea1b76ab957409a1cc2fbfba8a26/mingpt/bpe.py#L22-L33))ï¼Œæœ‰æ—¶æ˜¯éƒ¨åˆ†å•è¯(å¦‚ capes åˆ†ä¸º `Ä cap `å’Œ `es`)ï¼Œæœ‰æ—¶æ˜¯æ ‡ç‚¹ç¬¦å·(å¦‚ `.`)ã€‚

 BPE çš„ä¸€ä¸ªä¼˜ç‚¹æ˜¯å®ƒå¯ä»¥ç¼–ç ä»»æ„å­—ç¬¦ä¸²ï¼Œè‹¥é‡åˆ°è¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨çš„å†…å®¹ï¼Œå®ƒä¼šå°†å…¶åˆ†è§£ä¸ºå®ƒèƒ½ç†è§£çš„å­å­—ç¬¦ä¸²ï¼š

```python
[encoder.decoder[i] for i in encoder.encode("zjqfl")]
# ['z', 'j', 'q', 'fl']
```

**è¯æ±‡è¡¨(Vocabulary)**å’Œ**å­—èŠ‚å¯¹ç»„åˆ(Byte-pair merges)**æ˜¯ç°ä»£è‡ªç„¶è¯­è¨€å¤„ç†ä¸­**åˆ†è¯å™¨(Tokenizer)**çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚

è¯æ±‡è¡¨å°±åƒæ˜¯ä¸€æœ¬"å­—å…¸"ï¼Œå®ƒåŒ…å«äº†æ¨¡å‹èƒ½å¤Ÿç†è§£çš„æ‰€æœ‰"å•è¯"(tokens)åŠå…¶å¯¹åº”çš„æ•°å­— IDã€‚åœ¨ GPT æ¨¡å‹ä¸­ï¼Œè¿™äº›"å•è¯"å¯èƒ½æ˜¯çœŸå®çš„å•è¯ã€å•ä¸ªå­—ç¬¦ã€æˆ–è€…æ˜¯å¸¸è§çš„è¯ç»„ç‰‡æ®µã€‚

**å­—èŠ‚å¯¹ç¼–ç (BPEï¼ŒByte-Pair Encoding)** æ˜¯ä¸€ç§å†³å®šå¦‚ä½•å°†æ–‡æœ¬åˆ†è§£æˆ token çš„ç®—æ³•ã€‚å®ƒé¦–å…ˆå°†æ–‡æœ¬çœ‹ä½œå•ä¸ªå­—ç¬¦ï¼Œç„¶åé€æ­¥åˆå¹¶æœ€å¸¸ä¸€èµ·å‡ºç°çš„å­—ç¬¦å¯¹ï¼Œå½¢æˆæ–°çš„ tokenï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸æ–­é‡å¤ï¼Œç›´åˆ°è¾¾åˆ°é¢„è®¾çš„è¯æ±‡é‡ã€‚å‡è®¾"æœºå™¨å­¦ä¹ "è¿™ä¸ªè¯åœ¨è¯­æ–™åº“ä¸­ç»å¸¸å‡ºç°BPEç®—æ³•å¯èƒ½ä¼šå°†å…¶ä½œä¸ºä¸€ä¸ªå®Œæ•´çš„tokenï¼Œè€Œä¸æ˜¯åˆ†è§£ä¸º"æœº"ã€"å™¨"ã€"å­¦"ã€"ä¹ "å››ä¸ª token è¿™æ ·å¯ä»¥æ›´é«˜æ•ˆåœ°è¡¨ç¤ºå¸¸è§è¯ç»„ã€‚

> è¿™äº›æ–‡ä»¶åœ¨è¿è¡Œ `load_encoder_hparams_and_params`æ—¶è¢«ä¸‹è½½ã€‚å¯ä»¥æŸ¥çœ‹ `models/124M/encoder.json` (è¯æ±‡è¡¨)å’Œ `models/124M/vocab.bpe` (å­—èŠ‚å¯¹ç»„åˆ)ã€‚

`hparams `æ˜¯ä¸€ä¸ªåŒ…å«æˆ‘ä»¬æ¨¡å‹è¶…å‚æ•°çš„å­—å…¸ï¼š

```python
hparams
{ 
   "n_vocab": 50257, # è¯è¡¨ä¸­çš„ token æ•°é‡
    								 #
  									 # è™½ç„¶åœ¨è®­ç»ƒæ¨¡å‹æ—¶å·²ç»ç¡®å®šï¼Œä½†å®ƒä»ç„¶éœ€è¦ä½œä¸ºè¶…å‚æ•°:
                     # 1. æ¨¡å‹æ¶æ„éœ€è¦: æ¨¡å‹åœ¨åˆå§‹åŒ–å’Œè¿è¡Œæ—¶éœ€è¦çŸ¥é“è¯è¡¨çš„å¤§å°ï¼Œä»¥ä¾¿æ­£ç¡®è®¾ç½®åµŒå…¥å±‚çš„ç»´åº¦å’Œè¾“å‡ºå±‚çš„å¤§å°ï¼›
                     # 2. ä¸€è‡´æ€§æ£€æŸ¥: åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œéœ€è¦ç¡®ä¿ä½¿ç”¨çš„è¯è¡¨å¤§å°ä¸æ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨çš„è¯è¡¨å¤§å°ä¸€è‡´ï¼›
										 # 3. é…ç½®å®Œæ•´æ€§: å°†æ‰€æœ‰æ¨¡å‹ç›¸å…³çš„é…ç½®å‚æ•°ç»Ÿä¸€å­˜å‚¨åœ¨ä¸€ä¸ªåœ°æ–¹ï¼Œä½¿å¾—æ¨¡å‹çš„é…ç½®æ›´åŠ æ¸…æ™°å’Œå®Œæ•´ï¼›
  									 # 4. æ¨¡å‹å¤ç”¨: å½“éœ€è¦é‡æ–°åˆå§‹åŒ–æ¨¡å‹æˆ–è¿ç§»åˆ°æ–°ä»»åŠ¡æ—¶ï¼Œå®Œæ•´çš„è¶…å‚æ•°é›†åˆä½¿å¾—æ¨¡å‹ç»“æ„å¯ä»¥è¢«å‡†ç¡®åœ°é‡å»ºã€‚
  									 #
   "n_ctx"  : 1024,  # è¾“å…¥çš„æœ€å¤§å¯èƒ½åºåˆ—é•¿åº¦
    							   #
  									 # ä½œä¸ºè¶…å‚æ•°åŒ…å«åœ¨ hparams ä¸­æœ‰å‡ ä¸ªé‡è¦åŸå› ï¼š
  									 # 1. æ¨¡å‹ç»“æ„é™åˆ¶: Transformer æ¶æ„æœ‰ä¸€ä¸ªå›ºå®šçš„æœ€å¤§åºåˆ—é•¿åº¦é™åˆ¶ï¼Œè¿™æ˜¯æ¨¡å‹è®¾è®¡å’Œè®­ç»ƒæ—¶å°±ç¡®å®šçš„å‚æ•°ã€‚
								     # 2. ä½ç½®ç¼–ç : Transformer æ¨¡å‹ä½¿ç”¨ä½ç½®ç¼–ç (positional encoding)æ¥ç†è§£ token åœ¨åºåˆ—ä¸­çš„ä½ç½®ã€‚                      #             n_ctx å†³å®šäº†éœ€è¦å¤šå°‘ä¸åŒçš„ä½ç½®ç¼–ç ï¼Œè¿™ç›´æ¥å½±å“æ¨¡å‹çš„åˆå§‹åŒ–å’Œè®¡ç®—ã€‚
								     # 3. å†…å­˜åˆ†é…: æ¨¡å‹éœ€è¦é¢„å…ˆåˆ†é…è¶³å¤Ÿçš„å†…å­˜æ¥å¤„ç†æœ€å¤§é•¿åº¦çš„åºåˆ—ã€‚
								     # 4. æ³¨æ„åŠ›æœºåˆ¶è®¡ç®—: æ³¨æ„åŠ›æœºåˆ¶éœ€è¦è®¡ç®—åºåˆ—ä¸­æ¯ä¸ª token ä¸å…¶ä»–æ‰€æœ‰ token çš„å…³ç³»ï¼Œ
  									 #								  è®¡ç®—å¤æ‚åº¦ä¸åºåˆ—é•¿åº¦çš„å¹³æ–¹æˆæ­£æ¯”ã€‚ n_ctx é™åˆ¶äº†è¿™ç§è®¡ç®—çš„è§„æ¨¡ã€‚
								     # 5. è¿è¡Œæ—¶éªŒè¯ ï¼šåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œéœ€è¦éªŒè¯è¾“å…¥åºåˆ—æ˜¯å¦è¶…è¿‡äº†æ¨¡å‹èƒ½å¤„ç†çš„æœ€å¤§é•¿åº¦ï¼Œå¦‚ä»£ç ä¸­çš„æ£€æŸ¥ã€‚
  									 #
   "n_embd" : 768,   # åµŒå…¥ç»´åº¦(å†³å®šç½‘ç»œçš„"å®½åº¦")
   "n_head" : 12,    # æ³¨æ„åŠ›å¤´çš„æ•°é‡(n_embd å¿…é¡»èƒ½è¢« n_head æ•´é™¤)
   "n_layer": 12     # å±‚æ•°(å†³å®šç½‘ç»œçš„"æ·±åº¦")
}
```

æˆ‘ä»¬å°†åœ¨ä»£ç æ³¨é‡Šä¸­ä½¿ç”¨è¿™äº›ç¬¦å·æ¥å±•ç¤ºäº‹ç‰©çš„åº•å±‚ç»“æ„ã€‚æ­¤å¤–ï¼Œ æˆ‘ä»¬ä¼šä½¿ç”¨ `n_seq` è¡¨ç¤ºè¾“å…¥åºåˆ—çš„é•¿åº¦(å³`n_seq = len(inputs)`)ã€‚

`params `æ˜¯ä¸€ä¸ªåµŒå¥—çš„ JSON å­—å…¸ï¼Œç”¨äºä¿å­˜æ¨¡å‹çš„è®­ç»ƒæƒé‡ã€‚JSON çš„å¶èŠ‚ç‚¹æ˜¯ NumPy æ•°ç»„ã€‚å¦‚æœæ‰“å° `params`ï¼Œå¹¶å°†æ•°ç»„æ›¿æ¢ä¸ºå…¶å½¢çŠ¶ï¼Œæˆ‘ä»¬å°†å¾—åˆ°ï¼š

```python
import numpy as np
def shape_tree(d):
    if isinstance(d, np.ndarray):
        return list(d.shape)
    elif isinstance(d, list):
        return [shape_tree(v) for v in d]
    elif isinstance(d, dict):
        return {k: shape_tree(v) for k, v in d.items()}
    else:
        ValueError("uh oh")

print(shape_tree(params))
# {
#     "wpe": [ 1024, 768],
#     "wte": [50257, 768],
#     "ln_f": {"b": [768], "g": [768]},
#     "blocks": [
#         {
#             "attn": {
#                 "c_attn": {"b": [2304], "w": [768, 2304]},
#                 "c_proj": { "b": [768], "w": [768,  768]},
#             },
#             "ln_1": {"b": [768], "g": [768]},
#             "ln_2": {"b": [768], "g": [768]},
#             "mlp": {
#                 "c_fc"  : {"b": [3072], "w": [ 768, 3072]},
#                 "c_proj": {"b":  [768], "w": [3072,  768]},
#             },
#         },
#         ... # repeat for n_layers
#     ]
# }
```

è¿™äº›æ˜¯ä» OpenAI TensorFlow æ£€æŸ¥ç‚¹åŠ è½½çš„ï¼š

```python
import tensorflow as tf
tf_ckpt_path = tf.train.latest_checkpoint("models/124M")
for name, _ in tf.train.list_variables(tf_ckpt_path):
    arr = tf.train.load_variable(tf_ckpt_path, name).squeeze()
    print(f"{name}: {arr.shape}")
# model/h0/attn/c_attn/b: (2304,)
# model/h0/attn/c_attn/w: (768, 2304)
# model/h0/attn/c_proj/b: (768,)
# model/h0/attn/c_proj/w: (768, 768)
# model/h0/ln_1/b: (768,)
# model/h0/ln_1/g: (768,)
# model/h0/ln_2/b: (768,)
# model/h0/ln_2/g: (768,)
# model/h0/mlp/c_fc/b: (3072,)
# model/h0/mlp/c_fc/w: (768, 3072)
# model/h0/mlp/c_proj/b: (768,)
# model/h0/mlp/c_proj/w: (3072, 768)
# model/h1/attn/c_attn/b: (2304,)
# model/h1/attn/c_attn/w: (768, 2304)
...
# model/h9/mlp/c_proj/b: (768,)
# model/h9/mlp/c_proj/w: (3072, 768)
# model/ln_f/b: (768,)
# model/ln_f/g: (768,)
# model/wpe: (1024, 768)
# model/wte: (50257, 768)
```

ä¸ºäº†å¯¹æ¯”ï¼Œè¿™é‡Œæ˜¾ç¤ºäº† `params` çš„å½¢çŠ¶ï¼š

```python
{
    "wpe": [  n_ctx, n_embd],
    "wte": [n_vocab, n_embd],
    "ln_f": {"b": [n_embd], "g": [n_embd]},
    "blocks": [
        {
            "attn": {
                "c_attn": {"b": [3*n_embd], "w": [n_embd, 3*n_embd]},
                "c_proj": {"b": [  n_embd], "w": [n_embd,   n_embd]},
            },
            "ln_1": {"b": [n_embd], "g": [n_embd]},
            "ln_2": {"b": [n_embd], "g": [n_embd]},
            "mlp": {
                "c_fc"  : {"b": [4*n_embd], "w": [  n_embd, 4*n_embd]},
                "c_proj": {"b": [  n_embd], "w": [4*n_embd,   n_embd]},
            },
        },
        ... # repeat for n_layers
    ]
}
```

åœ¨å®ç° GPT æ—¶ï¼Œæˆ‘ä»¬éœ€è¦å›æ¥å‚è€ƒè¯¥å­—å…¸æ¥æ£€æŸ¥æƒé‡çš„å½¢çŠ¶ã€‚ä¸ºäº†ä¿æŒä¸€è‡´æ€§ï¼Œæˆ‘ä»¬ä¼šå°†ä»£ç ä¸­çš„å˜é‡åä¸å­—å…¸çš„é”®è¿›è¡ŒåŒ¹é…ã€‚

## åŸºç¡€çš„ç¥ç»ç½‘ç»œå±‚

åœ¨æ·±å…¥äº†è§£ GPT æ¶æ„æœ¬èº«ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆå®ç°ä¸€äº›åŸºç¡€çš„ç¥ç»ç½‘ç»œå±‚ï¼Œè¿™äº›å¹¶ä¸æ˜¯ GPT ç‰¹æœ‰çš„ã€‚

### é«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒ(GELU)

[GELU (Gaussian Error Linear Units)](https://huggingface.co/papers/1606.08415) æ˜¯ GPT-2 çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œåœ¨ Transformer æ¶æ„ä¸­è¡¨ç°ä¼˜äº ReLU å’Œå…¶ä»–æ¿€æ´»å‡½æ•°ã€‚ç¥ç»ç½‘ç»œä¸­æœ€åŸºæœ¬çš„æ“ä½œæ˜¯çº¿æ€§å˜æ¢(å¦‚çŸ©é˜µä¹˜æ³•å’Œåç½®åŠ æ³•)ã€‚å¦‚æœæ²¡æœ‰éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œæ— è®ºç¥ç»ç½‘ç»œæœ‰å¤šå°‘å±‚ï¼Œæ•´ä¸ªç½‘ç»œæœ¬è´¨ä¸Šä»ç„¶åªæ˜¯ä¸€ä¸ªçº¿æ€§æ¨¡å‹ã€‚éçº¿æ€§æ¿€æ´»å‡½æ•°æ‰“ç ´äº†è¿™ç§é™åˆ¶ï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„éçº¿æ€§å…³ç³»ã€‚

![Gaussian Error Linear Units Figure 1](./README.assets/gaussian_error_linear_units.png)

GELU æ¿€æ´»å‡½æ•° `GELU(x) = x * Î¦(x)` å¯ä»¥è¢«ç†è§£ä¸ºï¼šå°†è¾“å…¥å€¼ x ä¹˜ä»¥è¯¥è¾“å…¥è¢«ä¿ç•™çš„æ¦‚ç‡ã€‚è¿™ä¸ªæ¦‚ç‡ç”±æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°(CDF)ç»™å‡ºã€‚ç”±äºæ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ CDF è®¡ç®—å¤æ‚ï¼Œä»£ç ä½¿ç”¨äº†ä¸€ä¸ªå¸¸ç”¨çš„è¿‘ä¼¼å…¬å¼ï¼š

```python
def gelu(x):
  	return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))
```

GELU å¯¹è¾“å…¥è¿›è¡Œé€å…ƒç´ æ“ä½œï¼š

```python
gelu(np.array([[1, 2], [-2, 0.5]]))
# array([[ 0.84119,  1.9546],
#        [ -0.0454, 0.34571]])
```

### è½¯æœ€å¤§å€¼å‡½æ•°(Softmax)

Softmax å‡½æ•°åœ¨ç¥ç»ç½‘ç»œå’Œæ·±åº¦å­¦ä¹ ä¸­æ‰®æ¼”ç€éå¸¸é‡è¦çš„è§’è‰²ï¼ŒSoftmax çš„æ ¸å¿ƒä½œç”¨æ˜¯å°†ä¸€ç»„å®æ•°å€¼(é€šå¸¸ç§°ä¸º logits)è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚å®ƒç¡®ä¿æ‰€æœ‰è¾“å‡ºå€¼åœ¨ 0 åˆ° 1 ä¹‹é—´ï¼Œå¹¶ä¸”æ‰€æœ‰å€¼çš„æ€»å’Œä¸º 1ã€‚åœ¨ GPT-2 ç­‰è¯­è¨€æ¨¡å‹ä¸­ï¼ŒSoftmax ç”¨äºè¯æ±‡è¡¨ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒï¼Œå¸®åŠ©æ¨¡å‹é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªè¯ã€‚ä¸‹é¢æ˜¯æœ€ç»å…¸çš„ Softmax å‡½æ•°:
$$
\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}
$$

```python
def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    # é¦–å…ˆä»è¾“å…¥ x ä¸­å‡å»æ¯ä¸ªæ ·æœ¬çš„æœ€å¤§å€¼(æœ€å¤§çš„è¾“å…¥å€¼å˜ä¸º 0ï¼Œå…¶ä»–å€¼å˜ä¸ºè´Ÿæ•°)ï¼Œé˜²æ­¢æŒ‡æ•°è®¡ç®—æ—¶å‡ºç°æ•°å€¼æº¢å‡º
    # å¯¹è°ƒæ•´åçš„å€¼è®¡ç®—æŒ‡æ•° `exp_x = np.exp(x - max_x)`ï¼Œè¿™å°†æ‰€æœ‰å€¼è½¬æ¢ä¸ºæ­£æ•°
    # axis=-1 è¡¨ç¤ºæ²¿ç€æœ€åä¸€ä¸ªç»´åº¦æ“ä½œï¼Œå¯¹äº GPT-2 æ¨¡å‹çš„è¾“å‡º logitsï¼Œæœ€åä¸€ä¸ªç»´åº¦çš„å¤§å°ç­‰äºè¯æ±‡è¡¨å¤§å°
    # keepdims=True ä¿æŒæ•°ç»„çš„ç»´åº¦ç»“æ„ï¼Œä¾¿äºåç»­å¹¿æ’­æ“ä½œ
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)
    # è®¡ç®—æŒ‡æ•°å€¼çš„æ€»å’Œï¼Œå°†æ¯ä¸ªæŒ‡æ•°å€¼é™¤ä»¥æ€»å’Œï¼Œè¿™ç¡®ä¿è¾“å‡ºçš„æ‰€æœ‰å€¼åœ¨ 0 åˆ° 1 ä¹‹é—´ï¼Œä¸”æ€»å’Œä¸º 1ã€‚
```

```python
x = softmax(np.array([[2, 100], [-5, 0]]))
x
# array([[0.00034, 0.99966],
#        [0.26894, 0.73106]])
x.sum(axis=-1)
# array([1., 1.])
```

### å±‚å½’ä¸€åŒ–(Layer Normalization)

[å±‚å½’ä¸€åŒ–(Layer Normalization)](https://huggingface.co/papers/1607.06450) æ˜¯ä¸€ç§åœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­å¸¸ç”¨çš„æŠ€æœ¯ï¼Œé€šå¸¸åœ¨ç¥ç»ç½‘ç»œçš„æ¯ä¸€å±‚åº”ç”¨ï¼Œå¯¹æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾ç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–ã€‚å®ƒçš„ä¸»è¦ä½œç”¨æ˜¯åŠ é€Ÿç½‘ç»œè®­ç»ƒè¿‡ç¨‹ã€æé«˜æ¨¡å‹çš„ç¨³å®šæ€§ã€å‡è½»å†…éƒ¨åå˜é‡åç§»(Internal covariate shift)é—®é¢˜ã€ä½¿ç½‘ç»œå¯¹æƒé‡åˆå§‹åŒ–ä¸é‚£ä¹ˆæ•æ„Ÿç­‰ã€‚

å±‚å½’ä¸€åŒ–å°†æ•°æ®æ ‡å‡†åŒ–ä¸ºå‡å€¼ä¸º 0ã€æ–¹å·®ä¸º 1 çš„åˆ†å¸ƒï¼Œå…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š
$$
LayerNorm(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2}} + \beta
$$

```python
def layer_norm(x, g, b, eps: float = 1e-5):
    mean = np.mean(x, axis=-1, keepdims=True)    # å¯¹è¾“å…¥å¼ é‡ x åœ¨æœ€åä¸€ä¸ªç»´åº¦(é€šå¸¸æ˜¯ç‰¹å¾ç»´åº¦)ä¸Šè®¡ç®—å‡å€¼
    variance = np.var(x, axis=-1, keepdims=True) # åŒæ ·åœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Šè®¡ç®—æ–¹å·®
    x = (x - mean) / np.sqrt(variance + eps)     # å°†è¾“å…¥å‡å»å‡å€¼å¹¶é™¤ä»¥æ ‡å‡†å·®ï¼Œå®ç°æ ‡å‡†åŒ–
    																				     # eps æ˜¯ä¸€ä¸ªå°å¸¸æ•°(é»˜è®¤ä¸º1e-5)ï¼Œç”¨äºæ•°å€¼ç¨³å®šæ€§ï¼Œé˜²æ­¢é™¤ä»¥é›¶ã€‚
    return g * x + b # ä½¿ç”¨å¯å­¦ä¹ çš„å‚æ•° g(gamma)å’Œ b(beta)å¯¹æ ‡å‡†åŒ–åçš„æ•°æ®è¿›è¡Œçº¿æ€§å˜æ¢
  									 # è¿™ä½¿ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ æ¢å¤æ•°æ®çš„è¡¨ç¤ºèƒ½åŠ›
```

```python
x = np.array([[2, 2, 3], [-5, 0, 1]])
x = layer_norm(x, g=np.ones(x.shape[-1]), b=np.zeros(x.shape[-1]))
x
# array([[-0.70709, -0.70709,  1.41418],
#.       [  -1.397,    0.508,    0.889]])
x.var(axis=-1)
# array([0.99996, 1.])
x.mean(axis=-1)
#array([-0., -0.])
```

### çº¿æ€§å˜æ¢(Linear)

åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œçº¿æ€§å±‚é€šå¸¸ä¸éçº¿æ€§æ¿€æ´»å‡½æ•°ç»“åˆä½¿ç”¨ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚è¿™ç§ç»´åº¦å˜æ¢æ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„åŸºç¡€æ“ä½œï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒç»´åº¦çš„ç‰¹å¾ç©ºé—´ä¸­è¿›è¡Œè®¡ç®—å’Œå­¦ä¹ ã€‚

linear å‡½æ•°å®ç°äº†ä¸€ä¸ªæ ‡å‡†çš„çº¿æ€§å˜æ¢ï¼Œè¿™ä¸ªå‡½æ•°å®ç°äº†ä¸€ä¸ªæ ‡å‡†çš„çŸ©é˜µä¹˜æ³•åŠ åç½®çš„çº¿æ€§å±‚ï¼Œé€šå¸¸è¢«ç§°ä¸ºæŠ•å½±(Projection)ã€‚è¿™ä¸ªåç§°æ¥æºäºçº¿æ€§ä»£æ•°ä¸­çš„å‘é‡ç©ºé—´æŠ•å½±æ¦‚å¿µï¼Œå› ä¸ºå®ƒå°†å‘é‡ä»ä¸€ä¸ªå‘é‡ç©ºé—´æ˜ å°„(æˆ–"æŠ•å½±")åˆ°å¦ä¸€ä¸ªå‘é‡ç©ºé—´ï¼š

```python
def linear(x, w, b): 
  	# [m, in], [in, out], [out] -> [m, out]
    # x æ˜¯è¾“å…¥å‘é‡/çŸ©é˜µï¼Œå½¢çŠ¶ä¸º [m, in](mä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰inä¸ªç‰¹å¾)
    # w æ˜¯æƒé‡çŸ©é˜µï¼Œå½¢çŠ¶ä¸º [in, out](å°†inç»´è¾“å…¥æ˜ å°„åˆ°outç»´è¾“å‡º)
    # b æ˜¯åç½®å‘é‡ï¼Œå½¢çŠ¶ä¸º [out](ä¸ºæ¯ä¸ªè¾“å‡ºç»´åº¦æ·»åŠ ä¸€ä¸ªåç§»)
    # @ æ˜¯çŸ©é˜µä¹˜æ³•è¿ç®—ç¬¦
    # ç»“æœå½¢çŠ¶ä¸º [m, out](mä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰outä¸ªç‰¹å¾)
    return x @ w + b
```

```python
x = np.random.normal(size=(64, 784)) # è¡¨ç¤º 64 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰ 784 ä¸ªç‰¹å¾
w = np.random.normal(size=(784, 10)) # ç¬¬ä¸€ä¸ªç»´åº¦(784)å¿…é¡»åŒ¹é…è¾“å…¥çš„ç‰¹å¾ç»´åº¦ï¼Œç¬¬äºŒä¸ªç»´åº¦(10)å†³å®šäº†è¾“å‡ºçš„ç‰¹å¾æ•°é‡
b = np.random.normal(size=(10,))     # é•¿åº¦å¿…é¡»åŒ¹é…è¾“å‡ºçš„ç‰¹å¾æ•°é‡
x.shape # çº¿æ€§æŠ•å½±å‰çš„å½¢çŠ¶
# (64, 784)
linear(x, w, b).shape # çº¿æ€§æŠ•å½±åçš„å½¢çŠ¶
# (64, 10)
```

## GPT æ¶æ„

GPTçš„æ¶æ„æ˜¯åŸºäº [Transformer](https://huggingface.co/papers/1706.03762) çš„ï¼Œä½†å®ƒä»…ä»…ä½¿ç”¨äº†è§£ç å™¨å±‚ï¼Œæ­¤å¤–æˆ‘ä»¬å·²ç»æå®šäº†ç¼–ç å™¨ï¼Œæ‰€ä»¥ä¸­é—´çš„â€cross-attentionâ€å±‚è¢«ç§»é™¤ï¼š

| ![Attention Is All You Need Figure 1: The Transformer - model architecture](./README.assets/the_transformer_model_architecture_1.png) | ![Attention Is All You Need Figure 1: The Transformer - model architecture](./README.assets/the_transformer_model_architecture_2.png) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |

GPT æ¶æ„åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼šè¯å…ƒåµŒå…¥ (Word Token Embeddings) + ä½ç½®åµŒå…¥(WPE)ã€Transformer è§£ç å™¨å †æ ˆ(Decoder stack)ã€æŠ•å½±ä¸ºè¯æ±‡è¡¨(Projection to vocab)ã€‚åœ¨ä»£ç é‡Œå¦‚ä¸‹ï¼š

```python
def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):  # [n_seq] -> [n_seq, n_vocab]
  	# inputs : è¾“å…¥çš„ token åºåˆ—ï¼Œå½¢çŠ¶ä¸º [n_seq]
		#    wte : è¯å…ƒåµŒå…¥çŸ©é˜µ (Word Token Embeddings)
		#    wpe : ä½ç½®åµŒå…¥çŸ©é˜µ (Word Position Embeddings)
		# blocks : Transformerå—çš„åˆ—è¡¨
		#   ln_f : æœ€ç»ˆçš„å±‚å½’ä¸€åŒ–å‚æ•°
		# n_head : æ³¨æ„åŠ›å¤´çš„æ•°é‡
    
    #  + ä½ç½®åµŒå…¥ï¼šå°†è¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ª token è½¬æ¢ä¸ºåµŒå…¥å‘é‡ã€ä¸ºæ¯ä¸ªä½ç½®æ·»åŠ ä½ç½®ç¼–ç 
    x = wte[inputs] + wpe[range(len(inputs))]  # è¾“å…¥å½¢çŠ¶ä» [n_seq] å˜ä¸º [n_seq, n_embd]

    # é€šè¿‡ n å±‚ Transformer å—çš„å‰å‘ä¼ æ’­ï¼šä¾æ¬¡é€šè¿‡æ¯ä¸ª Transformer å—ã€æ¯ä¸ªå—ä¿æŒåµŒå…¥ç»´åº¦ä¸å˜
    for block in blocks:
        x = transformer_block(x, **block, n_head=n_head)  # è¾“å…¥å’Œè¾“å‡ºå½¢çŠ¶éƒ½æ˜¯ [n_seq, n_embd]

    # æŠ•å½±åˆ°è¯æ±‡è¡¨ï¼šå¯¹æœ€ç»ˆçš„éšè—çŠ¶æ€åº”ç”¨å±‚å½’ä¸€åŒ–ã€ä½¿ç”¨è¯å…ƒåµŒå…¥çŸ©é˜µçš„è½¬ç½®å°†éšè—çŠ¶æ€æŠ•å½±å›è¯æ±‡è¡¨ç©ºé—´
    x = layer_norm(x, **ln_f)  # è¾“å…¥å’Œè¾“å‡ºå½¢çŠ¶éƒ½æ˜¯ [n_seq, n_embd]
    return x @ wte.T  # è¾“å‡ºå½¢çŠ¶ä» [n_seq, n_embd] å˜ä¸º [n_seq, n_vocab]
```

è®©æˆ‘ä»¬æ›´è¯¦ç»†åœ°åˆ†è§£è¿™ä¸‰ä¸ªéƒ¨åˆ†ã€‚

### åµŒå…¥(Embeddings)

**è¯å…ƒåµŒå…¥çŸ©é˜µ (Word Token Embeddings)**

Token æœ¬èº«å¹¶ä¸èƒ½å¾ˆå¥½åœ°è¡¨å¾ç¥ç»ç½‘ç»œã€‚é¦–å…ˆï¼ŒToken çš„ç›¸å¯¹å¤§å°ä¼šé”™è¯¯åœ°ä¼ è¾¾ä¿¡æ¯(è‹¥è¯æ±‡è¡¨ä¸­æœ‰ `Apple = 5` å’Œ `Table = 10`ï¼Œä½†å¹¶ä¸æ„å‘³ç€ `2 * Apple = Table`)ã€‚å…¶æ¬¡ï¼Œå•ä¸ªæ•°å­—å¯¹äºç¥ç»ç½‘ç»œæ¥è¯´ç»´åº¦ä¸å¤Ÿé«˜ã€‚

ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å°†åˆ©ç”¨[è¯å‘é‡(Word vector)](https://jaykmody.com/blog/attention-intuition/#word-vectors-and-similarity)ï¼Œé€šè¿‡å­¦ä¹ åµŒå…¥çŸ©é˜µï¼Œå°†ç¦»æ•£çš„ token è½¬æ¢ä¸ºè¿ç»­çš„å‘é‡è¡¨ç¤ºï¼š

```python
wte[inputs] # [n_seq] -> [n_seq, n_embd]
```

`wte` æ˜¯ä¸€ä¸ª `[n_vocab, n_embd]` çŸ©é˜µã€‚å®ƒå……å½“æŸ¥æ‰¾è¡¨ï¼ŒçŸ©é˜µä¸­çš„ç¬¬ i è¡Œå¯¹åº”è¯æ±‡è¡¨ä¸­çš„ç¬¬ i ä¸ª token çš„å‘é‡è¡¨ç¤ºã€‚`wte[inputs]` ä½¿ç”¨æ•´æ•°æ•°ç»„ç´¢å¼•æ¥æ£€ç´¢ä¸è¾“å…¥ä¸­çš„æ¯ä¸ª token ç›¸å¯¹åº”çš„å‘é‡ã€‚ä¸ç¥ç»ç½‘ç»œä¸­çš„å…¶ä»–å‚æ•°ä¸€æ ·ï¼Œ`wte` æ˜¯å­¦ä¹ è€Œæ¥çš„ã€‚å®ƒåœ¨è®­ç»ƒå¼€å§‹æ—¶éšæœºåˆå§‹åŒ–ï¼Œç„¶åé€šè¿‡æ¢¯åº¦ä¸‹é™è¿›è¡Œæ›´æ–°ã€‚

è¿™äº›è¿ç»­çš„å‘é‡è¡¨ç¤ºæ•æ‰äº†è¯å…ƒçš„è¯­ä¹‰ä¿¡æ¯ï¼šè¯­ä¹‰ç›¸ä¼¼çš„è¯å…ƒåœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»è¾ƒè¿‘ï¼›å‘é‡ä¹‹é—´å¯ä»¥è¿›è¡Œæ•°å­¦è¿ç®—ï¼Œå¦‚ vec("å›½ç‹") - vec("ç”·äºº") + vec("å¥³äºº") â‰ˆ vec("ç‹å")ï¼›è¿™äº›å‘é‡ä½œä¸ºæ¨¡å‹çš„è¾“å…¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç†è§£å’Œå¤„ç†è‡ªç„¶è¯­è¨€ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒGPT-2 æ¨¡å‹å°†ç¦»æ•£çš„ã€è®¡ç®—æœºæ˜“äºå¤„ç†çš„ token è½¬æ¢ä¸ºè¿ç»­çš„ã€åŒ…å«ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯çš„å‘é‡è¡¨ç¤ºï¼Œä¸ºåç»­çš„ç¥ç»ç½‘ç»œå¤„ç†å¥ å®šåŸºç¡€ã€‚

**ä½ç½®åµŒå…¥çŸ©é˜µ(WPE)**

Transformer æ¶æ„çš„ä¸€ä¸ªå¥‡ç‰¹ä¹‹å¤„åœ¨äºå®ƒæœ¬èº«ä¸è€ƒè™‘è¾“å…¥åºåˆ—çš„ä½ç½®ä¿¡æ¯ã€‚å¦‚æœæˆ‘ä»¬éšæœºæ‰“ä¹±è¾“å…¥åºåˆ—ä¸­è¯å…ƒçš„é¡ºåºï¼Œæ¨¡å‹çš„è¾“å‡ºå¯èƒ½ä¿æŒä¸å˜ï¼Œè¿™æ„å‘³ç€è¾“å…¥çš„é¡ºåºå¯¹è¾“å‡ºæ²¡æœ‰å½±å“ã€‚ç„¶è€Œï¼Œåœ¨è‡ªç„¶è¯­è¨€ä¸­ï¼Œè¯çš„é¡ºåºæ˜¾ç„¶æ˜¯è‡³å…³é‡è¦çš„ã€‚ä¾‹å¦‚ï¼Œ"çŒ«è¿½ç‹—"å’Œ"ç‹—è¿½çŒ«"è¡¨è¾¾çš„æ˜¯å®Œå…¨ä¸åŒçš„æ„æ€ï¼Œå°½ç®¡ä½¿ç”¨äº†ç›¸åŒçš„è¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦æŸç§æ–¹å¼å°†ä½ç½®ä¿¡æ¯ç¼–ç åˆ°è¾“å…¥ä¸­ã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒGPT-2ä½¿ç”¨äº†ä¸€ä¸ªå¯å­¦ä¹ çš„ä½ç½®åµŒå…¥çŸ©é˜µï¼š

```python
wpe[range(len(inputs))] # [n_seq] -> [n_seq, n_embd]
```

`wpe` æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º `[n_ctx, n_embd]` çš„çŸ©é˜µï¼ŒçŸ©é˜µçš„ç¬¬ i è¡ŒåŒ…å«ä¸€ä¸ªå‘é‡ï¼Œç”¨äºç¼–ç è¾“å…¥åºåˆ—ä¸­ç¬¬ i ä¸ªä½ç½®çš„ä¿¡æ¯ï¼Œä¸è¯å…ƒåµŒå…¥çŸ©é˜µç±»ä¼¼ï¼Œè¿™ä¸ªä½ç½®åµŒå…¥çŸ©é˜µä¹Ÿæ˜¯é€šè¿‡æ¢¯åº¦ä¸‹é™å­¦ä¹ å¾—åˆ°çš„ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™ç§æ–¹æ³•é™åˆ¶äº†æ¨¡å‹èƒ½å¤„ç†çš„æœ€å¤§åºåˆ—é•¿åº¦ä¸º `n_ctx` ã€‚è¿™æ„å‘³ç€å¿…é¡»æ»¡è¶³æ¡ä»¶ï¼š `len(inputs) <= n_ctx`ã€‚è¿™æ˜¯å› ä¸ºä½ç½®åµŒå…¥çŸ©é˜µ `wpe` åªåŒ…å«äº† `n_ctx` ä¸ªä¸åŒä½ç½®çš„åµŒå…¥å‘é‡ã€‚å¦‚æœè¾“å…¥åºåˆ—é•¿åº¦è¶…è¿‡ `n_ctx`ï¼Œæ¨¡å‹å°†æ— æ³•ä¸ºè¶…å‡ºéƒ¨åˆ†çš„ä½ç½®æä¾›æœ‰æ•ˆçš„ä½ç½®ç¼–ç ã€‚åœ¨ GPT-2 ä¸­ï¼Œ `n_ctx` é€šå¸¸è®¾ç½®ä¸º1024ï¼Œè¿™æ„å‘³ç€æ¨¡å‹ä¸€æ¬¡æœ€å¤šå¯ä»¥å¤„ç† 1024ä¸ª token çš„åºåˆ—ã€‚

**ç»„åˆ**

åœ¨ GPT-2 æ¨¡å‹ä¸­ï¼Œè¾“å…¥è¡¨ç¤ºæ˜¯é€šè¿‡ç»„åˆä¸¤ç§ä¸åŒç±»å‹çš„åµŒå…¥æ¥åˆ›å»ºçš„ï¼šè¯å…ƒåµŒå…¥å’Œä½ç½®åµŒå…¥ã€‚è¿™æ®µä»£ç å±•ç¤ºäº†è¿™ä¸€å…³é”®æ­¥éª¤ï¼š

```python
# è¯å…ƒåµŒå…¥ + ä½ç½®åµŒå…¥
x = wte[inputs] + wpe[range(len(inputs))]  # [n_seq] -> [n_seq, n_embd]
# x[i] å°†ä¸¤ä¸ªç›¸åŒå½¢çŠ¶çš„çŸ©é˜µå¯¹åº”å…ƒç´ ç›¸åŠ ï¼Œç»“æœ x çš„å½¢çŠ¶ä»ä¸º [n_seq, n_embd]
```

å¯¹äºè¾“å…¥åºåˆ—ä¸­çš„ç¬¬ i ä¸ªè¯å…ƒï¼Œ x[i] åŒ…å«äº†è¯¥è¯å…ƒçš„è¯­ä¹‰ä¿¡æ¯å’Œå®ƒåœ¨åºåˆ—ä¸­ç¬¬ i ä¸ªä½ç½®çš„ä½ç½®ä¿¡æ¯ï¼Œå³ä½¿ç›¸åŒçš„è¯å…ƒå‡ºç°åœ¨ä¸åŒä½ç½®ï¼Œå®ƒä»¬çš„æœ€ç»ˆè¡¨ç¤ºä¹Ÿä¼šä¸åŒæ¨¡å‹èƒ½å¤Ÿç†è§£"çŒ«è¿½ç‹—"å’Œ"ç‹—è¿½çŒ«"çš„åŒºåˆ«ï¼Œå› ä¸ºè™½ç„¶è¯å…ƒç›¸åŒï¼Œä½†ä½ç½®åµŒå…¥ä¸åŒã€‚è¿™ç§ç»„åˆåµŒå…¥æ–¹æ³•æ˜¯ GPT-2 ç­‰ Transformer æ¨¡å‹æˆåŠŸå¤„ç†åºåˆ—æ•°æ®çš„å…³é”®æŠ€æœ¯ä¹‹ä¸€ã€‚

### è§£ç å™¨å †æ ˆ(Decoder Stack)

è¿™æ®µä»£ç æ˜¯ GPT-2 æ¨¡å‹ä¸­æœ€æ ¸å¿ƒçš„éƒ¨åˆ†ï¼Œä¹Ÿæ˜¯æ·±åº¦å­¦ä¹ ä¸­"æ·±åº¦"çš„å…·ä½“ä½“ç°ï¼š

```python
# é€šè¿‡ n_layer ä¸ª Transformer è§£ç å™¨å—çš„å‰å‘ä¼ æ’­
for block in blocks:
    x = transformer_block(x, **block, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]
```

è¿™é‡Œçš„"æ·±åº¦"æŒ‡çš„æ˜¯ç¥ç»ç½‘ç»œä¸­å±‚çš„æ•°é‡ã€‚åœ¨è¿™ä¸ªå¾ªç¯ä¸­ï¼Œè¾“å…¥æ•°æ® x ä¾æ¬¡é€šè¿‡å¤šä¸ª Transformer è§£ç å™¨å—ï¼Œæ¯ä¸ªå—éƒ½å¯¹æ•°æ®è¿›è¡Œä¸€ç³»åˆ—å¤æ‚çš„è½¬æ¢ï¼Œç„¶åå°†ç»“æœä¼ é€’ç»™ä¸‹ä¸€ä¸ªå—ã€‚

æ¯ä¸ª Transformer å—é€šå¸¸åŒ…å«ï¼šå¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€å‰é¦ˆç¥ç»ç½‘ç»œã€æ®‹å·®è¿æ¥ã€å±‚å½’ä¸€åŒ–ã€‚

å †å æ›´å¤šçš„ Transformer å—(å¢åŠ  n_layer çš„å€¼)å¯ä»¥ï¼šå¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´å¤æ‚çš„æ¨¡å¼å’Œå…³ç³»ã€æé«˜æ¨¡å‹å¤„ç†é•¿è·ç¦»ä¾èµ–çš„èƒ½åŠ›ã€‚éšç€æ·±åº¦(å±‚æ•°)å’Œå®½åº¦(åµŒå…¥ç»´åº¦)çš„å¢åŠ ï¼Œæ¨¡å‹çš„å‚æ•°æ•°é‡å’Œè®¡ç®—å¤æ‚åº¦ä¹Ÿä¼šæ˜¾è‘—å¢åŠ ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå¤§å‹è¯­è¨€æ¨¡å‹éœ€è¦å¼ºå¤§çš„è®¡ç®—èµ„æºè¿›è¡Œè®­ç»ƒå’Œæ¨ç†çš„åŸå› ã€‚

æ€»ä¹‹ï¼Œè¿™ä¸ªç®€å•çš„å¾ªç¯æ˜¯ GPT-2 æ¨¡å‹èƒ½åŠ›çš„æ ¸å¿ƒæ‰€åœ¨ï¼Œé€šè¿‡å †å å¤šä¸ª Transformer å—ï¼Œæ¨¡å‹èƒ½å¤Ÿé€å±‚æ„å»ºå¯¹è¾“å…¥åºåˆ—çš„æ·±å…¥ç†è§£ï¼Œå¹¶æœ€ç»ˆç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬è¾“å‡ºã€‚

### è¯æ±‡æŠ•å½±(Projection to Vocab)

è¯æ±‡æŠ•å½±æ˜¯ GPT-2 æ¨¡å‹ä¸­çš„æœ€åä¸€ä¸ªå…³é”®æ­¥éª¤ï¼Œå®ƒå°† Transformer å—çš„è¾“å‡ºè½¬æ¢ä¸ºè¯æ±‡è¡¨ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒï¼š

```python
# æŠ•å½±åˆ°è¯æ±‡è¡¨
x = layer_norm(x, **ln_f)  # [n_seq, n_embd] -> [n_seq, n_embd]
return x @ wte.T  # [n_seq, n_embd] -> [n_seq, n_vocab]
```

å…¶å·¥ä½œåŸç†æ˜¯ï¼š

1. æœ€ç»ˆå±‚å½’ä¸€åŒ–ï¼šåœ¨æŠ•å½±ä¹‹å‰ï¼Œé¦–å…ˆå¯¹ Transformer å—çš„è¾“å‡ºåº”ç”¨å±‚å½’ä¸€åŒ–ï¼Œè¿™æ˜¯ GPT-2 æ¶æ„ç‰¹æœ‰çš„è®¾è®¡(åŸå§‹ GPT å’Œ Transformer è®ºæ–‡ä¸­æ²¡æœ‰)ï¼Œå±‚å½’ä¸€åŒ–æœ‰åŠ©äºç¨³å®šæ·±å±‚ç½‘ç»œçš„è®­ç»ƒå’Œæ¨ç†ï¼›
2. çŸ©é˜µä¹˜æ³•æŠ•å½±ï¼šä½¿ç”¨è¯å…ƒåµŒå…¥çŸ©é˜µçš„è½¬ç½®(wte.T)è¿›è¡ŒæŠ•å½±ï¼Œè¾“å…¥å½¢çŠ¶ä¸º `[n_seq, n_embd]`ï¼Œè¾“å‡ºå½¢çŠ¶ä¸º `[n_seq, n_vocab]`æ¯ä¸ªä½ç½®çš„è¾“å‡ºå‘é‡åŒ…å«è¯æ±‡è¡¨ä¸­æ¯ä¸ª token çš„ logit åˆ†æ•°ã€‚

æƒé‡å…±äº«å­˜åœ¨ä¸€å®šçš„ä¼˜åŠ¿ï¼š

GPT-2 é‡ç”¨è¯å…ƒåµŒå…¥çŸ©é˜µ(wte)è¿›è¡ŒæŠ•å½±ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å•ç‹¬çš„æƒé‡çŸ©é˜µï¼š

1. å‚æ•°èŠ‚çº¦ï¼šå‡å°‘äº†æ¨¡å‹çš„å‚æ•°æ•°é‡(è™½ç„¶åœ¨GPT-3è§„æ¨¡ä¸Šè¿™ç§èŠ‚çº¦å¯ä»¥å¿½ç•¥ä¸è®¡)

2. æ›´ä¸°å¯Œçš„è¡¨ç¤ºï¼šåŒä¸€çŸ©é˜µæ—¢è´Ÿè´£ä» token æ˜ å°„åˆ°åµŒå…¥ç©ºé—´ï¼Œä¹Ÿè´Ÿè´£ä»åµŒå…¥ç©ºé—´æ˜ å°„å› token ç©ºé—´ç†è®ºä¸Šï¼Œè¿™ç§åŒå‘æ˜ å°„å¯ä»¥å­¦ä¹ åˆ°æ¯”ä¸¤ä¸ªç‹¬ç«‹çŸ©é˜µæ›´ä¸°å¯Œçš„è¡¨ç¤ºã€‚

å…¶ä¸­ï¼Œæ¨¡å‹è¾“å‡ºçš„æ˜¯logits(æœªå½’ä¸€åŒ–çš„åˆ†æ•°)ï¼Œè€Œä¸æ˜¯åº”ç”¨softmaxåçš„æ¦‚ç‡ï¼š

1. è´ªå©ªé‡‡æ ·çš„ç­‰æ•ˆæ€§ï¼šç”±äº softmax æ˜¯å•è°ƒå‡½æ•°ï¼Œ `np.argmax(logits)` ç­‰åŒäº `np.argmax(softmax(logits))`ï¼Œå¯¹äºè´ªå©ªé‡‡æ ·æ¥è¯´ï¼Œåº”ç”¨ softmaxæ˜¯å¤šä½™çš„ï¼›

2. ä¿¡æ¯ä¿ç•™ï¼šlogits åŒ…å«æ›´å¤šä¿¡æ¯ï¼Œå¯ä»¥éšæ—¶é€šè¿‡åº”ç”¨ softmax è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œä½†ä»æ¦‚ç‡æ— æ³•æ¢å¤å› logitsï¼Œå› æ­¤è¾“å‡º logits æä¾›äº†æœ€å¤§çš„çµæ´»æ€§ï¼›

3. æ•°å€¼ç¨³å®šæ€§ï¼šåœ¨è®¡ç®—æŸå¤±å‡½æ•°æ—¶(å¦‚äº¤å‰ç†µæŸå¤±)ï¼Œç›´æ¥ä½¿ç”¨logitsé€šå¸¸æ›´ç¨³å®šï¼Œ`log(softmax(logits))` å¯ä»¥ç”¨æ›´ç¨³å®šçš„ `log_softmax(logits)` æ›¿ä»£ã€‚

æŠ•å½±åˆ°è¯æ±‡è¡¨çš„æ­¥éª¤æœ‰æ—¶è¢«ç§°ä¸º**è¯­è¨€å»ºæ¨¡å¤´(language modeling head)**ï¼š

1. "å¤´"çš„å«ä¹‰ï¼šæ¨¡å‹å¯ä»¥æœ‰å¤šä¸ªä¸åŒç±»å‹çš„è¾“å‡ºå±‚("å¤´")ï¼›
2. çµæ´»æ€§ï¼šé¢„è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥å°†è¯­è¨€å»ºæ¨¡å¤´æ›¿æ¢ä¸ºå…¶ä»–ç±»å‹çš„æŠ•å½±ï¼›
3. è¿ç§»å­¦ä¹ ï¼šä¾‹å¦‚ï¼Œå¯ä»¥æ·»åŠ åˆ†ç±»å¤´ç”¨äºç‰¹å®šä»»åŠ¡çš„å¾®è°ƒï¼›
4. å¤šä»»åŠ¡èƒ½åŠ›ï¼šå°±åƒç¥è¯ä¸­çš„ä¹å¤´è›‡ä¸€æ ·ï¼Œæ¨¡å‹å¯ä»¥æœ‰å¤šä¸ª"å¤´"æ¥å¤„ç†ä¸åŒä»»åŠ¡ã€‚

è¿™ç§è®¾è®¡ä½¿ GPT-2 æ¨¡å‹åœ¨å®Œæˆé¢„è®­ç»ƒåèƒ½å¤Ÿçµæ´»åœ°é€‚åº”å„ç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œè€Œä¸ä»…ä»…å±€é™äºè¯­è¨€ç”Ÿæˆã€‚

### è§£ç å™¨å—(Decoder Block)

Transformer è§£ç å™¨å—æ˜¯ GPT-2 ç­‰æ¨¡å‹çš„æ ¸å¿ƒç»„ä»¶ï¼Œå®ƒç”±ä¸¤ä¸ªä¸»è¦å­å±‚ç»„æˆï¼š

1. å¤šå¤´å› æœè‡ªæ³¨æ„åŠ›æœºåˆ¶(Multi-head causal self attention)ï¼›
2. ä½ç½®å¼å‰é¦ˆç¥ç»ç½‘ç»œ(Position-wise feed forward neural network)ã€‚

è®©æˆ‘ä»¬è¯¦ç»†åˆ†æ `transformer_block` å‡½æ•°çš„å®ç°ï¼š

```python
def transformer_block(x, mlp, attn, ln_1, ln_2, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]
    # å¤šå¤´å› æœè‡ªæ³¨æ„åŠ›
    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]
    # ä½ç½®å¼å‰é¦ˆç¥ç»ç½‘ç»œ
    x = x + ffn(layer_norm(x, **ln_2), **mlp)  # [n_seq, n_embd] -> [n_seq, n_embd]
    return x
```

æ¯ä¸ªå­å±‚éƒ½åœ¨è¾“å…¥ä¸Šä½¿ç”¨äº†å±‚å½’ä¸€åŒ–ï¼Œä¹Ÿä½¿ç”¨äº†æ®‹å·®è¿æ¥(å³å°†å­å±‚çš„è¾“å…¥ç›´æ¥è¿æ¥åˆ°å­å±‚çš„è¾“å‡º)ã€‚éœ€è¦æ³¨æ„çš„äº‹é¡¹ï¼š

1. å¤šå¤´å› æœè‡ªæ³¨æ„åŠ›æœºåˆ¶ä¿ƒè¿›äº†è¾“å…¥ä¹‹é—´çš„é€šä¿¡ã€‚åœ¨ Transformer æ¶æ„ä¸­ï¼Œåªæœ‰æ³¨æ„åŠ›æœºåˆ¶å…è®¸ä¸åŒä½ç½®çš„ token äº’ç›¸"çœ‹åˆ°"å¯¹æ–¹å¹¶äº¤æ¢ä¿¡æ¯ã€‚å¤§å¤šæ•°ç»„ä»¶å°±åƒ"ç‹¬ç«‹å·¥ä½œè€…"ï¼Œåªå…³æ³¨è‡ªå·±ä½ç½®çš„æ•°æ®å¤„ç†ï¼šåµŒå…¥å±‚(å°†æ¯ä¸ª token ç‹¬ç«‹è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºï¼Œä¸å…³å¿ƒå…¶ä»–tokenæ˜¯ä»€ä¹ˆ)ã€ä½ç½®å‰é¦ˆç½‘ç»œ(å¯¹åºåˆ—ä¸­æ¯ä¸ªä½ç½®ç‹¬ç«‹åº”ç”¨ç›¸åŒçš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œæ¯ä¸ªä½ç½®çš„è®¡ç®—ä¸å…¶ä»–ä½ç½®æ— å…³)ã€å±‚å½’ä¸€åŒ–(å¯¹æ¯ä¸ªä½ç½®çš„ç‰¹å¾ç‹¬ç«‹è¿›è¡Œå½’ä¸€åŒ–ï¼Œä¸è€ƒè™‘åºåˆ—ä¸­å…¶ä»–ä½ç½®çš„å€¼)ã€è¯æ±‡æŠ•å½±(å°†æ¯ä¸ªä½ç½®çš„å‘é‡ç‹¬ç«‹æ˜ å°„å›è¯æ±‡è¡¨æ¦‚ç‡ã€æ¯ä¸ªä½ç½®çš„é¢„æµ‹ä¸ç›´æ¥ä¾èµ–å…¶ä»–ä½ç½®)ï¼Œè€Œæ³¨æ„åŠ›æœºåˆ¶åˆ™åƒ"ç¤¾äº¤åè°ƒè€…"ï¼Œè´Ÿè´£è®©ä¸åŒä½ç½®çš„ä¿¡æ¯ç›¸äº’äº¤æµå’Œå½±å“(è´Ÿè´£æ•æ‰åºåˆ—ä¸­çš„é•¿è·ç¦»ä¾èµ–å’Œå¤æ‚å…³ç³»ï¼Œé€šè¿‡è®¡ç®—ä¸åŒä½ç½®ä¹‹é—´çš„æ³¨æ„åŠ›æƒé‡å®ç°ä¿¡æ¯äº¤æµ)ã€‚

2. ä½ç½®å¼å‰é¦ˆç¥ç»ç½‘ç»œåªæ˜¯ä¸€ä¸ªæ™®é€šçš„ä¸¤å±‚å…¨è¿æ¥ç¥ç»ç½‘ç»œã€‚å®ƒåªæ˜¯ä¸ºæˆ‘ä»¬çš„æ¨¡å‹å¢åŠ äº†ä¸€äº›å¯å­¦ä¹ çš„å‚æ•°ï¼Œä»¥ä¿ƒè¿›å­¦ä¹ ã€‚
3. åœ¨åŸå§‹çš„ Transformer è®ºæ–‡ä¸­ï¼Œå±‚å½’ä¸€åŒ–è¢«æ”¾ç½®åœ¨è¾“å‡ºä¸Šï¼š`layer_norm(x + sublayer(x))`ï¼Œè€Œåœ¨GPT-2ä¸­ï¼Œæˆ‘ä»¬å°†å±‚å½’ä¸€åŒ–æ”¾ç½®åœ¨è¾“å…¥ä¸Šï¼š`x + sublayer(layer_norm(x))`ï¼Œè¿™ç§æ–¹å¼è¢«ç§°ä¸º"å‰ç½®å½’ä¸€åŒ–"(Pre-Norm)ï¼Œ[å·²è¢«è¯æ˜å¯¹æå‡ Transformer çš„æ€§èƒ½éå¸¸é‡è¦](https://huggingface.co/papers/2002.04745)ã€‚


4. æ®‹å·®è¿æ¥(Residual connectionsï¼Œç”± [ResNet](https://huggingface.co/papers/1512.03385) æ¨å¹¿)æœ‰å‡ ä¸ªä¸åŒçš„ä½œç”¨ï¼š
   1. ä½¿æ·±å±‚ç¥ç»ç½‘ç»œæ›´å®¹æ˜“ä¼˜åŒ– ï¼šä¸ºæ¢¯åº¦æä¾›äº†"æ·å¾„"ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å®¹æ˜“åœ°æµå›ç½‘ç»œï¼›è¿™ä½¿å¾—ä¼˜åŒ–ç½‘ç»œçš„æ—©æœŸå±‚å˜å¾—æ›´åŠ å®¹æ˜“ï¼›
   2. è§£å†³æ·±åº¦é€€åŒ–é—®é¢˜ ï¼šæ²¡æœ‰æ®‹å·®è¿æ¥æ—¶ï¼Œå½“æ·»åŠ æ›´å¤šå±‚æ—¶ï¼Œæ›´æ·±çš„æ¨¡å‹ä¼šå‡ºç°æ€§èƒ½ä¸‹é™ï¼Œæ®‹å·®è¿æ¥ä¼¼ä¹ä¸ºæ›´æ·±çš„ç½‘ç»œæä¾›äº†ä¸€å®šçš„å‡†ç¡®åº¦æå‡ï¼›
   3. å¸®åŠ©è§£å†³æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é—®é¢˜ ï¼šé€šè¿‡æä¾›ç›´æ¥è·¯å¾„ï¼Œå‡è½»äº†æ·±å±‚ç½‘ç»œä¸­å¸¸è§çš„æ¢¯åº¦é—®é¢˜ï¼Œä½¿å¾—ä¿¡å·èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä¼ æ’­ã€‚

#### ä½ç½®å¼å‰é¦ˆç¥ç»ç½‘ç»œ

"ä½ç½®å¼"æ„å‘³ç€è¿™ä¸ªå‰é¦ˆç½‘ç»œç‹¬ç«‹åœ°åº”ç”¨äºåºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®ã€‚å¯¹äºè¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ªtokenï¼Œéƒ½ä½¿ç”¨å®Œå…¨ç›¸åŒçš„å‰é¦ˆç½‘ç»œè¿›è¡Œå¤„ç†ï¼Œä¸”å„ä¸ªä½ç½®ä¹‹é—´çš„å¤„ç†æ˜¯ç›¸äº’ç‹¬ç«‹çš„ã€‚

è¿™ä¸ªå‡½æ•°å®ç°äº†ä¸€ä¸ªä¸¤å±‚çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼š

```python
def ffn(x, c_fc, c_proj):  # [n_seq, n_embd] -> [n_seq, n_embd] 
    # å‘ä¸ŠæŠ•å½±
    # å°†è¾“å…¥ x é€šè¿‡çº¿æ€§å˜æ¢ linear(x, **c_fc) æ˜ å°„åˆ°ä¸€ä¸ªæ›´é«˜ç»´åº¦çš„ç©ºé—´
    # è¾“å…¥ç»´åº¦ä» [n_seq, n_embd] å˜ä¸º [n_seq, 4*n_embd] (ç»´åº¦æ‰©å¤§äº†4å€)
    # ç„¶ååº”ç”¨GELUæ¿€æ´»å‡½æ•° gelu() å¼•å…¥éçº¿æ€§
    a = gelu(linear(x, **c_fc))
    # å‘ä¸‹æŠ•å½±
    # å°†æ¿€æ´»åçš„ç»“æœ a é€šè¿‡å¦ä¸€ä¸ªçº¿æ€§å˜æ¢ linear(a, **c_proj) æ˜ å°„å›åŸå§‹ç»´åº¦
    # è¾“å‡ºç»´åº¦ä» [n_seq, 4*n_embd] å˜å› [n_seq, n_embd]
    x = linear(a, **c_proj)
    return x
```

è¿™ä¸ª ffn å‡½æ•°å®ç°äº† Transformer æ¶æ„ä¸­çš„ä½ç½®å¼å‰é¦ˆç¥ç»ç½‘ç»œï¼Œå®ƒé€šè¿‡å…ˆæ‰©å±•ç»´åº¦ã€åº”ç”¨éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œç„¶åå†å‹ç¼©å›åŸå§‹ç»´åº¦çš„æ–¹å¼ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚è™½ç„¶ç»“æ„ç®€å•ï¼Œä½†å®ƒæ˜¯ Transformer æ¨¡å‹ä¸­ä¸å¯æˆ–ç¼ºçš„ç»„æˆéƒ¨åˆ†ã€‚

> åœ¨Transformeræ¶æ„ä¸­ï¼Œå‰é¦ˆç½‘ç»œé€šå¸¸å°†ç»´åº¦æ‰©å±•åˆ°åŸæ¥çš„4å€ï¼Œè¿™æ˜¯ä¸€ç§ç»éªŒæ€§çš„è®¾è®¡é€‰æ‹©ï¼š
>
> 1. å¢åŠ æ¨¡å‹å®¹é‡ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ æ›´å¤æ‚çš„ç‰¹å¾ï¼›
> 2. æä¾›è¶³å¤Ÿçš„è¡¨è¾¾èƒ½åŠ›ï¼Œæ•æ‰æ›´ä¸°å¯Œçš„è¯­è¨€æ¨¡å¼ï¼›
> 3. åœ¨å®è·µä¸­è¢«è¯æ˜æ˜¯ä¸€ä¸ªè¾ƒå¥½çš„å¹³è¡¡ç‚¹(åœ¨æ¨¡å‹å¤æ‚åº¦å’Œæ€§èƒ½ä¹‹é—´)ã€‚

å›å¿†ä¸€ä¸‹æˆ‘ä»¬çš„ `params` å­—å…¸ï¼Œæˆ‘ä»¬çš„ `mlp`å‚æ•°å¦‚ä¸‹ï¼š

```python
"mlp": {
    "c_fc"  : {"b": [4*n_embd], "w": [  n_embd, 4*n_embd]},
    "c_proj": {"b":   [n_embd], "w": [4*n_embd,   n_embd]}
}
```

params å­—å…¸ä¸­çš„ mlp å‚æ•°ç›´æ¥å¯¹åº”äº† ffn å‡½æ•°ä¸­çš„çº¿æ€§å˜æ¢å‚æ•°ï¼Œå®ƒä»¬å…±åŒå®ç°äº† Transformer æ¶æ„ä¸­çš„ä½ç½®å¼å‰é¦ˆç¥ç»ç½‘ç»œç»„ä»¶ã€‚

#### å¤šå¤´å› æœè‡ªæ³¨æ„åŠ›æœºåˆ¶

è¿™ä¸€å±‚å¯èƒ½æ˜¯Transformerä¸­æœ€éš¾ç†è§£çš„éƒ¨åˆ†ã€‚è®©æˆ‘ä»¬é€šè¿‡é€ä¸ªåˆ†è§£æ¯ä¸ªè¯æ¥ç†è§£"å¤šå¤´å› æœè‡ªæ³¨æ„åŠ›"ï¼šæ³¨æ„åŠ›(Attention)ã€è‡ªæ³¨æ„åŠ›(Self-Attention)ã€å› æœ(Causal)ã€å¤šå¤´(Multi-Head)ã€‚

##### æ³¨æ„åŠ›(Attention)

æ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹å…³æ³¨è¾“å…¥åºåˆ—ä¸­çš„ç‰¹å®šéƒ¨åˆ†ï¼Œè€Œä¸æ˜¯å¹³ç­‰åœ°å¤„ç†æ‰€æœ‰è¾“å…¥ã€‚åœ¨Transformerä¸­ï¼Œè¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•æ‰åºåˆ—ä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»å’Œå¤æ‚æ¨¡å¼ã€‚

> å¯ä»¥å‚è€ƒ [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/) å’Œ [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)ï¼Œè¿™äº›éƒ½æ˜¯å…³äºæ³¨æ„åŠ›æœºåˆ¶çš„ä¼˜ç§€è§£é‡Šã€‚

è¿™ä¸ªå‡½æ•°å®ç°äº†ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶ï¼š
$$
\text{attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V
$$

```python
def attention(q, k, v):  # [n_q, d_k], [n_k, d_k], [n_k, d_v] -> [n_q, d_v]
  	# qï¼šæŸ¥è¯¢çŸ©é˜µ(Query)ï¼Œå½¢çŠ¶ä¸º [n_q, d_k] ï¼Œå…¶ä¸­ n_q æ˜¯æŸ¥è¯¢åºåˆ—é•¿åº¦ï¼Œ d_k æ˜¯é”®çš„ç»´åº¦
		# kï¼šé”®çŸ©é˜µ(Key)ï¼Œå½¢çŠ¶ä¸º [n_k, d_k] ï¼Œå…¶ä¸­ n_k æ˜¯é”®åºåˆ—é•¿åº¦
    # vï¼šå€¼çŸ©é˜µ(Value)ï¼Œå½¢çŠ¶ä¸º [n_k, d_v] ï¼Œå…¶ä¸­ d_v æ˜¯å€¼çš„ç»´åº¦
    return softmax(q @ k.T / np.sqrt(q.shape[-1])) @ v
```

1. è¾“å…¥å‚æ•° ï¼š

   - `q`ï¼šæŸ¥è¯¢çŸ©é˜µ(Query)ï¼Œå½¢çŠ¶ä¸º [n_q, d_k] ï¼Œå…¶ä¸­ n_q æ˜¯æŸ¥è¯¢åºåˆ—é•¿åº¦ï¼Œ d_k æ˜¯é”®çš„ç»´åº¦
   - `k`ï¼šé”®çŸ©é˜µ(Key)ï¼Œå½¢çŠ¶ä¸º [n_k, d_k] ï¼Œå…¶ä¸­ n_k æ˜¯é”®åºåˆ—é•¿åº¦
   - `v`ï¼šå€¼çŸ©é˜µ(Value)ï¼Œå½¢çŠ¶ä¸º [n_k, d_v] ï¼Œå…¶ä¸­ d_v æ˜¯å€¼çš„ç»´åº¦
2. è®¡ç®—æ­¥éª¤ ï¼š

   - `q @ k.T`ï¼šè®¡ç®—æŸ¥è¯¢å’Œé”®çš„ç‚¹ç§¯ï¼Œå¾—åˆ°æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µï¼Œå½¢çŠ¶ä¸º [n_q, n_k]
   - `/np.sqrt(q.shape[-1])` ï¼šé™¤ä»¥é”®ç»´åº¦çš„å¹³æ–¹æ ¹è¿›è¡Œç¼©æ”¾ï¼Œè¿™æ˜¯ä¸ºäº†é˜²æ­¢ç‚¹ç§¯å€¼è¿‡å¤§å¯¼è‡´softmaxæ¢¯åº¦æ¶ˆå¤±
   - `softmax(...)`ï¼šå¯¹ç¼©æ”¾åçš„æ³¨æ„åŠ›åˆ†æ•°åº”ç”¨softmaxå‡½æ•°ï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡
   - `... @ v`ï¼šç”¨æ³¨æ„åŠ›æƒé‡å¯¹å€¼è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå¾—åˆ°æœ€ç»ˆçš„æ³¨æ„åŠ›è¾“å‡ºï¼Œå½¢çŠ¶ä¸º [n_q, d_v]

è¿™ä¸ªç®€æ´çš„ attention å‡½æ•°æ˜¯Transformeræ¶æ„çš„æ ¸å¿ƒï¼Œå®ƒé€šè¿‡è®¡ç®—æŸ¥è¯¢ä¸é”®çš„ç›¸ä¼¼åº¦ï¼Œå¹¶ç”¨è¿™äº›ç›¸ä¼¼åº¦å¯¹å€¼è¿›è¡ŒåŠ æƒï¼Œå®ç°äº†åºåˆ—ä¸­ä¸åŒä½ç½®ä¹‹é—´çš„ä¿¡æ¯äº¤æµã€‚è¿™ç§æœºåˆ¶æ˜¯ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹(å¦‚GPTç³»åˆ—)å¼ºå¤§èƒ½åŠ›çš„åŸºç¡€ã€‚

##### è‡ªæ³¨æ„åŠ›(Self-Attention)

å½“æŸ¥è¯¢(q)ã€é”®(k)å’Œå€¼(v)éƒ½æ¥è‡ªåŒä¸€ä¸ªæºæ—¶ï¼Œæˆ‘ä»¬æ‰§è¡Œçš„æ˜¯è‡ªæ³¨æ„åŠ›(è®©è¾“å…¥åºåˆ—å…³æ³¨è‡ªèº«)ï¼š

```python
def self_attention(x): # [n_seq, n_embd] -> [n_seq, n_embd] 
    return attention(q=x, k=x, v=x) 
```

ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬çš„è¾“å…¥æ˜¯"Jayå»å•†åº—ï¼Œä»–ä¹°äº†10ä¸ªè‹¹æœ"ï¼Œæˆ‘ä»¬ä¼šè®©å•è¯"ä»–"å…³æ³¨æ‰€æœ‰å…¶ä»–å•è¯ï¼ŒåŒ…æ‹¬"Jay"ï¼Œè¿™æ„å‘³ç€æ¨¡å‹å¯ä»¥å­¦ä¹ è¯†åˆ«"ä»–"æŒ‡çš„æ˜¯"Jay"ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸º qã€kã€v å’Œæ³¨æ„åŠ›è¾“å‡ºå¼•å…¥æŠ•å½±çŸ©é˜µæ¥å¢å¼ºè‡ªæ³¨æ„åŠ›ï¼š

```python
def self_attention(x, w_k, w_q, w_v, w_proj): # [n_seq, n_embd] -> [n_seq, n_embd] 
    # qkvæŠ•å½±
    q = x @ w_q # [n_seq, n_embd] @ [n_embd, n_embd] -> [n_seq, n_embd] 
    k = x @ w_k # [n_seq, n_embd] @ [n_embd, n_embd] -> [n_seq, n_embd] 
    v = x @ w_v # [n_seq, n_embd] @ [n_embd, n_embd] -> [n_seq, n_embd] 
    
    # æ‰§è¡Œè‡ªæ³¨æ„åŠ›
    x = attention(q, k, v) # [n_seq, n_embd] -> [n_seq, n_embd] 
    
    # è¾“å‡ºæŠ•å½±
    x = x @ w_proj # [n_seq, n_embd] @ [n_embd, n_embd] -> [n_seq, n_embd] 
    
    return x 
```

è¿™ä½¿æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿå­¦ä¹  qã€k å’Œ v çš„æ˜ å°„ï¼Œä»¥æœ€å¥½åœ°å¸®åŠ©æ³¨æ„åŠ›åŒºåˆ†è¾“å…¥ä¹‹é—´çš„å…³ç³»ã€‚

æˆ‘ä»¬å¯ä»¥å°†çŸ©é˜µä¹˜æ³•ä» 4 æ¬¡å‡å°‘åˆ°åªæœ‰ 2 æ¬¡ï¼Œæ–¹æ³•æ˜¯å°† w_qã€w_k å’Œ w_v åˆå¹¶ä¸ºå•ä¸ªçŸ©é˜µ w_fcï¼Œæ‰§è¡ŒæŠ•å½±ï¼Œç„¶åæ‹†åˆ†ç»“æœï¼š

```python
def self_attention(x, w_fc, w_proj): # [n_seq, n_embd] -> [n_seq, n_embd] 
    # qkvæŠ•å½±
    x = x @ w_fc # [n_seq, n_embd] @ [n_embd, 3*n_embd] -> [n_seq, 3*n_embd] 
    
    # æ‹†åˆ†ä¸ºqkv
    q, k, v = np.split(x, 3, axis=-1) # [n_seq, 3*n_embd] -> 3ä¸ª [n_seq, n_embd] 
    
    # æ‰§è¡Œè‡ªæ³¨æ„åŠ›
    x = attention(q, k, v) # [n_seq, n_embd] -> [n_seq, n_embd] 
    
    # è¾“å‡ºæŠ•å½±
    x = x @ w_proj # [n_seq, n_embd] @ [n_embd, n_embd] = [n_seq, n_embd] 
    
    return x 
```

è¿™æ›´é«˜æ•ˆï¼Œå› ä¸ºç°ä»£åŠ é€Ÿå™¨(GPU)å¯ä»¥æ›´å¥½åœ°åˆ©ç”¨ä¸€ä¸ªå¤§å‹çŸ©é˜µä¹˜æ³•ï¼Œè€Œä¸æ˜¯é¡ºåºå‘ç”Ÿçš„ 3 ä¸ªå•ç‹¬çš„å°çŸ©é˜µä¹˜æ³•ã€‚

æœ€åï¼Œæˆ‘ä»¬æ·»åŠ åç½®å‘é‡ä»¥åŒ¹é… GPT-2 çš„å®ç°ï¼Œä½¿ç”¨çº¿æ€§å‡½æ•°ï¼Œå¹¶é‡å‘½åå‚æ•°ä»¥åŒ¹é…æˆ‘ä»¬çš„ params å­—å…¸ï¼š

```python
def self_attention(x, c_attn, c_proj): # [n_seq, n_embd] -> [n_seq, n_embd] 
    # qkvæŠ•å½±
    x = linear(x, **c_attn) # [n_seq, n_embd] -> [n_seq, 3*n_embd] 
    
    # æ‹†åˆ†ä¸ºqkv
    q, k, v = np.split(x, 3, axis=-1) # [n_seq, 3*n_embd] -> 3ä¸ª [n_seq, n_embd] 
    
    # æ‰§è¡Œè‡ªæ³¨æ„åŠ›
    x = attention(q, k, v) # [n_seq, n_embd] -> [n_seq, n_embd] 
    
    # è¾“å‡ºæŠ•å½±
    x = linear(x, **c_proj) # [n_seq, n_embd] @ [n_embd, n_embd] = [n_seq, n_embd] 
    
    return x 
```

```python
"attn": { 
    "c_attn": {"b": [3*n_embd], "w": [n_embd, 3*n_embd]}, 
    "c_proj": {"b":   [n_embd], "w": [n_embd,   n_embd]},
},
```

è¿™ç§å®ç°æ–¹å¼æ—¢é«˜æ•ˆåˆç¬¦åˆ GPT-2 çš„åŸå§‹è®¾è®¡ï¼Œé€šè¿‡åˆå¹¶çŸ©é˜µä¹˜æ³•å’Œæ·»åŠ é€‚å½“çš„åç½®ï¼Œä¼˜åŒ–äº†è®¡ç®—è¿‡ç¨‹ã€‚

##### å› æœ(Causal)

æˆ‘ä»¬å½“å‰çš„è‡ªæ³¨æ„åŠ›è®¾ç½®å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼šè¾“å…¥å¯ä»¥"çœ‹åˆ°"æœªæ¥ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬çš„è¾“å…¥æ˜¯["not", "all", "heroes", "wear", "capes"]ï¼Œåœ¨è‡ªæ³¨æ„åŠ›è®¡ç®—ä¸­ï¼Œæˆ‘ä»¬å…è®¸"wear"çœ‹åˆ°"capes"ã€‚è¿™æ„å‘³ç€"wear"çš„è¾“å‡ºæ¦‚ç‡ä¼šæœ‰åå·®ï¼Œå› ä¸ºæ¨¡å‹å·²ç»çŸ¥é“æ­£ç¡®ç­”æ¡ˆæ˜¯"capes"ã€‚è¿™ä¸å¥½ï¼Œå› ä¸ºæˆ‘ä»¬çš„æ¨¡å‹åªä¼šå­¦ä¹ åˆ°è¾“å…¥ i+1 çš„æ­£ç¡®ç­”æ¡ˆå¯ä»¥ç›´æ¥ä»è¾“å…¥ i+1 è·å–ã€‚

ä¸ºäº†é˜²æ­¢è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹æ³¨æ„åŠ›çŸ©é˜µï¼Œä½¿è¾“å…¥æ— æ³•çœ‹åˆ°æœªæ¥ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬çš„æ³¨æ„åŠ›çŸ©é˜µå¦‚ä¸‹ï¼š

```python
       not    all    heroes wear   capes 
   not 0.116  0.159  0.055  0.226  0.443 
   all 0.180  0.397  0.142  0.106  0.175 
heroes 0.156  0.453  0.028  0.129  0.234 
  wear 0.499  0.055  0.133  0.017  0.295 
 capes 0.089  0.290  0.240  0.228  0.153 
```

æ¯è¡Œå¯¹åº”ä¸€ä¸ªæŸ¥è¯¢(query)ï¼Œåˆ—å¯¹åº”ä¸€ä¸ªé”®(key)ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒæŸ¥çœ‹"wear"é‚£ä¸€è¡Œï¼Œä½ å¯ä»¥çœ‹åˆ°å®ƒåœ¨æœ€åä¸€åˆ—å¯¹"capes"çš„æ³¨æ„åŠ›æƒé‡ä¸º0.295ã€‚ä¸ºäº†é˜²æ­¢è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬å¸Œæœ›å°†è¯¥æ¡ç›®è®¾ç½®ä¸º0.0ï¼š

```python
       not    all    heroes wear   capes 
   not 0.116  0.159  0.055  0.226  0.443 
   all 0.180  0.397  0.142  0.106  0.175 
heroes 0.156  0.453  0.028  0.129  0.234 
  wear 0.499  0.055  0.133  0.017  0. 
 capes 0.089  0.290  0.240  0.228  0.153 
```

ä¸€èˆ¬æ¥è¯´ï¼Œä¸ºäº†é˜²æ­¢è¾“å…¥ä¸­çš„æ‰€æœ‰æŸ¥è¯¢çœ‹åˆ°æœªæ¥ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰ä½ç½® j>i çš„å€¼è®¾ä¸º 0ï¼š

```python
       not    all    heroes wear   capes 
   not 0.116  0.     0.     0.     0. 
   all 0.180  0.397  0.     0.     0. 
heroes 0.156  0.453  0.028  0.     0. 
  wear 0.499  0.055  0.133  0.017  0. 
 capes 0.089  0.290  0.240  0.228  0.153 
```

æˆ‘ä»¬ç§°è¿™ä¸ºæ©ç (Masking)ã€‚

ä¸Šè¿°æ©ç æ–¹æ³•æœ‰ä¸€ä¸ªé—®é¢˜ï¼šæˆ‘ä»¬çš„è¡Œä¸å†æ€»å’Œä¸º1(å› ä¸ºæˆ‘ä»¬åœ¨ softmax åº”ç”¨åå°†å®ƒä»¬è®¾ç½®ä¸º0)ã€‚ä¸ºäº†ç¡®ä¿è¡Œä»ç„¶æ€»å’Œä¸º1ï¼Œæˆ‘ä»¬éœ€è¦åœ¨åº”ç”¨softmaxä¹‹å‰ä¿®æ”¹æ³¨æ„åŠ›çŸ©é˜µã€‚è¿™å¯ä»¥é€šè¿‡åœ¨ softmax ä¹‹å‰å°†è¦æ©ç çš„æ¡ç›®è®¾ç½®ä¸º -âˆ æ¥å®ç°ï¼š

```python
def attention(q, k, v, mask):  # [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -> [n_q, d_v] 
    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v 
```

å…¶ä¸­maskæ˜¯çŸ©é˜µ(å¯¹äºn_seq=5)ï¼š

```python
0 -1e10 -1e10 -1e10 -1e10 
0   0   -1e10 -1e10 -1e10 
0   0     0   -1e10 -1e10 
0   0     0     0   -1e10 
0   0     0     0     0 
```

æˆ‘ä»¬ä½¿ç”¨ -1e10 è€Œä¸æ˜¯ -np.infï¼Œå› ä¸º -np.inf å¯èƒ½å¯¼è‡´ NaN å€¼ã€‚å°† mask æ·»åŠ åˆ°æˆ‘ä»¬çš„æ³¨æ„åŠ›çŸ©é˜µè€Œä¸æ˜¯ç›´æ¥å°†å€¼è®¾ç½®ä¸º -1e10 æ˜¯æœ‰æ•ˆçš„ï¼Œå› ä¸ºå®é™…ä¸Šï¼Œä»»ä½•æ•°åŠ ä¸Š -âˆ ä»ç„¶æ˜¯ -âˆã€‚æˆ‘ä»¬å¯ä»¥ç”¨ NumPy è®¡ç®—æ©ç çŸ©é˜µï¼š` (1 - np.tri(n_seq)) * -1e10`ã€‚

å°†æ‰€æœ‰å†…å®¹æ”¾åœ¨ä¸€èµ·ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š

```python
def attention(q, k, v, mask):  # [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -> [n_q, d_v] 
    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v 

def causal_self_attention(x, c_attn, c_proj): # [n_seq, n_embd] -> [n_seq, n_embd] 
    # qkvæŠ•å½±
    x = linear(x, **c_attn) # [n_seq, n_embd] -> [n_seq, 3*n_embd] 

    # æ‹†åˆ†ä¸ºqkv
    q, k, v = np.split(x, 3, axis=-1) # [n_seq, 3*n_embd] -> 3ä¸ª [n_seq, n_embd] 

    # å› æœæ©ç ï¼Œé˜²æ­¢æœªæ¥è¾“å…¥è¢«å…³æ³¨
    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10  # [n_seq, n_seq] 

    # æ‰§è¡Œå› æœè‡ªæ³¨æ„åŠ›
    x = attention(q, k, v, causal_mask) # [n_seq, n_embd] -> [n_seq, n_embd] 

    # è¾“å‡ºæŠ•å½±
    x = linear(x, **c_proj) # [n_seq, n_embd] @ [n_embd, n_embd] = [n_seq, n_embd] 

    return x
```

##### å¤šå¤´(Multi-Head)

æˆ‘ä»¬å¯ä»¥é€šè¿‡æ‰§è¡Œ n_head ä¸ªç‹¬ç«‹çš„æ³¨æ„åŠ›è®¡ç®—æ¥è¿›ä¸€æ­¥æ”¹è¿›æˆ‘ä»¬çš„å®ç°ï¼Œå°†æŸ¥è¯¢ã€é”®å’Œå€¼åˆ†å‰²æˆå¤šä¸ªå¤´ï¼š

```python
def mha(x, c_attn, c_proj, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd] 
    # qkvæŠ•å½± 
    x = linear(x, **c_attn)  # [n_seq, n_embd] -> [n_seq, 3*n_embd] 

    # æ‹†åˆ†ä¸ºqkv 
    qkv = np.split(x, 3, axis=-1)  # [n_seq, 3*n_embd] -> [3, n_seq, n_embd] 

    # æ‹†åˆ†ä¸ºå¤šå¤´ 
    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  # [3, n_seq, n_embd] -> [3, n_head, n_seq, n_embd/n_head] 

    # å› æœæ©ç ï¼Œé˜²æ­¢æœªæ¥è¾“å…¥è¢«å…³æ³¨ 
    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10  # [n_seq, n_seq] 

    # å¯¹æ¯ä¸ªå¤´æ‰§è¡Œæ³¨æ„åŠ›è®¡ç®— 
    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]  # [3, n_head, n_seq, n_embd/n_head] -> [n_head, n_seq, n_embd/n_head] 

    # åˆå¹¶å¤šå¤´ç»“æœ 
    x = np.hstack(out_heads)  # [n_head, n_seq, n_embd/n_head] -> [n_seq, n_embd] 

    # è¾“å‡ºæŠ•å½± 
    x = linear(x, **c_proj)  # [n_seq, n_embd] -> [n_seq, n_embd] 

    return x 
```

è¿™é‡Œæ·»åŠ äº†ä¸‰ä¸ªæ­¥éª¤ï¼š

1. å°†qã€kã€væ‹†åˆ†ä¸ºn_headä¸ªå¤´ ï¼š

```python
# æ‹†åˆ†ä¸ºå¤šå¤´ 
qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  # [3, n_seq, n_embd] -> [n_head, 3, n_seq, n_embd/n_head] 
```

2. ä¸ºæ¯ä¸ªå¤´è®¡ç®—æ³¨æ„åŠ› ï¼š

```python
# å¯¹æ¯ä¸ªå¤´æ‰§è¡Œæ³¨æ„åŠ›è®¡ç®— 
out_heads = [attention(q, k, v) for q, k, v in zip(*qkv_heads)]  # [n_head, 3, n_seq, n_embd/n_head] -> [n_head, n_seq, n_embd/n_head] 
```

3. åˆå¹¶æ¯ä¸ªå¤´çš„è¾“å‡º ï¼š

```python
# åˆå¹¶å¤šå¤´ç»“æœ 
x = np.hstack(out_heads)  # [n_head, n_seq, n_embd/n_head] -> [n_seq, n_embd] 
```

æ³¨æ„ï¼Œè¿™å°†æ¯ä¸ªæ³¨æ„åŠ›è®¡ç®—çš„ç»´åº¦ä»n_embdé™ä½åˆ°n_embd/n_headã€‚è¿™æ˜¯ä¸€ç§æƒè¡¡ã€‚é€šè¿‡é™ä½ç»´åº¦ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è·å¾—äº†é¢å¤–çš„å­ç©ºé—´æ¥å¤„ç†é€šè¿‡æ³¨æ„åŠ›å»ºæ¨¡çš„å…³ç³»ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªæ³¨æ„åŠ›å¤´å¯èƒ½è´Ÿè´£è¿æ¥ä»£è¯ä¸ä»£è¯æ‰€æŒ‡çš„äººã€‚å¦ä¸€ä¸ªå¯èƒ½è´Ÿè´£é€šè¿‡å¥å·å¯¹å¥å­è¿›è¡Œåˆ†ç»„ã€‚è¿˜æœ‰ä¸€ä¸ªå¯èƒ½åªæ˜¯è¯†åˆ«å“ªäº›è¯æ˜¯å®ä½“ï¼Œå“ªäº›ä¸æ˜¯ã€‚å½“ç„¶ï¼Œè¿™å¯èƒ½åªæ˜¯å¦ä¸€ä¸ªç¥ç»ç½‘ç»œçš„é»‘ç›’å­ã€‚

æˆ‘ä»¬ç¼–å†™çš„ä»£ç åœ¨å¾ªç¯ä¸­é¡ºåºæ‰§è¡Œæ¯ä¸ªå¤´çš„æ³¨æ„åŠ›è®¡ç®—(ä¸€æ¬¡ä¸€ä¸ª)ï¼Œè¿™ä¸æ˜¯å¾ˆé«˜æ•ˆã€‚åœ¨å®è·µä¸­ï¼Œä½ ä¼šå¸Œæœ›å¹¶è¡Œæ‰§è¡Œè¿™äº›è®¡ç®—ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬åªä¿ç•™è¿™ç§é¡ºåºæ‰§è¡Œçš„æ–¹å¼ã€‚

è‡³æ­¤ï¼Œæˆ‘ä»¬ç»ˆäºå®Œæˆäº†GPTçš„å®ç°ï¼ç°åœ¨ï¼Œå‰©ä¸‹çš„å°±æ˜¯æŠŠæ‰€æœ‰å†…å®¹æ”¾åœ¨ä¸€èµ·å¹¶è¿è¡Œæˆ‘ä»¬çš„ä»£ç ã€‚

## è¿è¡Œ GPT

å°†æ‰€æœ‰å†…å®¹æ•´åˆåœ¨ä¸€èµ·ï¼Œæˆ‘ä»¬å¾—åˆ°äº†gpt2.pyï¼Œæ•´ä¸ªæ–‡ä»¶ä»…æœ‰120è¡Œä»£ç (å¦‚æœå»æ‰æ³¨é‡Šå’Œç©ºç™½è¡Œï¼Œåªæœ‰60è¡Œ)ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æµ‹è¯•æˆ‘ä»¬çš„å®ç°ï¼š

```bash
python gpt2.py \
    "Alan Turing theorized that computers would one day become" \
    --n_tokens_to_generate 8
```

è¿™ä¼šè¾“å‡ºï¼š

```
the most powerful machines on the planet.
```

# æ¥ä¸‹æ¥åšä»€ä¹ˆ

## GPU/TPU æ”¯æŒ

JAX æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„æ•°å€¼è®¡ç®—åº“ï¼Œå®ƒç»“åˆäº† NumPy çš„æ˜“ç”¨æ€§å’Œ XLA(åŠ é€Ÿçº¿æ€§ä»£æ•°)çš„æ€§èƒ½ã€‚JAX çš„ä¸»è¦ç‰¹ç‚¹æ˜¯ï¼š

1. ä¸ NumPy æ¥å£å…¼å®¹ ï¼šJAX æä¾›äº†ä¸ NumPy å‡ ä¹ç›¸åŒçš„ APIï¼Œä½¿å¾—ä»£ç è¿ç§»éå¸¸ç®€å•ï¼›
2. è‡ªåŠ¨å¾®åˆ† ï¼šæ”¯æŒè‡ªåŠ¨æ±‚å¯¼ï¼Œå¯¹æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒéå¸¸æœ‰ç”¨ï¼›
3. ç¡¬ä»¶åŠ é€Ÿ ï¼šåŸç”Ÿæ”¯æŒ GPU å’Œ TPUï¼Œå¯ä»¥æ˜¾è‘—æé«˜è®¡ç®—é€Ÿåº¦ï¼›
4. å³æ—¶ç¼–è¯‘ ï¼šä½¿ç”¨ XLA ç¼–è¯‘å™¨ä¼˜åŒ–è®¡ç®—å›¾ã€‚

å°†æˆ‘ä»¬çš„ GPT-2 å®ç°ä» NumPy è½¬æ¢ä¸º JAX åªéœ€è¦ä¸€è¡Œä»£ç çš„æ”¹å˜ï¼š

```python
# åŸæ¥çš„å¯¼å…¥
import numpy as np

# æ”¹ä¸º
import jax.numpy as np
```

ç”±äº JAX çš„ NumPy API ä¸åŸå§‹ NumPy é«˜åº¦å…¼å®¹ï¼Œæˆ‘ä»¬ä¸éœ€è¦ä¿®æ”¹ä»»ä½•å…¶ä»–ä»£ç ã€‚è¿™ç§ç®€å•çš„æ›¿æ¢å°±èƒ½è®©æˆ‘ä»¬çš„å®ç°åˆ©ç”¨ GPU/TPU çš„è®¡ç®—èƒ½åŠ›ï¼Œå¤§å¤§åŠ é€Ÿæ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚

## åå‘ä¼ æ’­(Backpropagation)

å¦‚æœæˆ‘ä»¬å°†NumPyæ›¿æ¢ä¸ºJAXï¼Œé‚£ä¹ˆè®¡ç®—æ¢¯åº¦å°±å˜å¾—éå¸¸ç®€å•ï¼š

```python
def lm_loss(params, inputs, n_head) -> float:
    x, y = inputs[:-1], inputs[1:]  # è¾“å…¥åºåˆ—å’Œç›®æ ‡åºåˆ—
    logits = gpt2(x, **params, n_head=n_head)  # è·å–æ¨¡å‹è¾“å‡º
    loss = np.mean(-log_softmax(logits)[y])  # è®¡ç®—äº¤å‰ç†µæŸå¤±
    return loss

grads = jax.grad(lm_loss)(params, inputs, n_head)
```

è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ï¼š`jax.grad(lm_loss)` åˆ›å»ºä¸€ä¸ªæ–°å‡½æ•°ï¼Œè¯¥å‡½æ•°è®¡ç®— `lm_loss` ç›¸å¯¹äºå…¶ç¬¬ä¸€ä¸ªå‚æ•°(è¿™é‡Œæ˜¯ params )çš„æ¢¯åº¦`(params, inputs, n_head)` å°†å‚æ•°ä¼ é€’ç»™è¿™ä¸ªæ–°åˆ›å»ºçš„æ¢¯åº¦å‡½æ•°ç»“æœ `grads` åŒ…å«äº†æ¨¡å‹æ‰€æœ‰å‚æ•°ç›¸å¯¹äºæŸå¤±å‡½æ•°çš„æ¢¯åº¦ã€‚è¿™ç§æ–¹æ³•çš„ä¼˜ç‚¹æ˜¯ï¼šæ— éœ€æ‰‹åŠ¨å®ç°åå‘ä¼ æ’­ç®—æ³•ã€JAX è‡ªåŠ¨å¤„ç†è®¡ç®—å›¾çš„æ„å»ºå’Œæ¢¯åº¦è®¡ç®—ã€ä»£ç ç®€æ´æ˜äº†ï¼Œåªéœ€å‡ è¡Œå°±èƒ½å®ç°å®Œæ•´çš„æ¢¯åº¦è®¡ç®—ã€‚

## æ‰¹å¤„ç†(Batching)

å¦‚æœæˆ‘ä»¬å°†NumPyæ›¿æ¢ä¸ºJAXï¼Œé‚£ä¹ˆï¼Œè®©æˆ‘ä»¬çš„gpt2å‡½æ•°æ”¯æŒæ‰¹å¤„ç†å°±å˜å¾—éå¸¸ç®€å•ï¼š

```python
gpt2_batched = jax.vmap(gpt2, in_axes=[0, None, None, None, None, None])
gpt2_batched(batched_inputs) # [batch, seq_len] -> [batch, seq_len, vocab]
```

JAXçš„ vmap å‡½æ•°(å‘é‡åŒ–æ˜ å°„)æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œå®ƒå¯ä»¥è‡ªåŠ¨å°†å‡½æ•°è½¬æ¢ä¸ºæ”¯æŒæ‰¹å¤„ç†çš„ç‰ˆæœ¬ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼š

1. `jax.vmap(gpt2, in_axes=[0, None, None, None, None, None])` åˆ›å»ºäº†ä¸€ä¸ªæ–°å‡½æ•° `gpt2_batched`
2. `in_axes=[0, None, None, None, None, None]` æŒ‡å®šäº†å“ªäº›å‚æ•°åº”è¯¥è¢«æ‰¹å¤„ç†ï¼š
   1. 0 è¡¨ç¤ºç¬¬ä¸€ä¸ªå‚æ•°( inputs )åº”è¯¥åœ¨ç¬¬0ç»´åº¦ä¸Šæ‰¹å¤„ç†
   2. None è¡¨ç¤ºå…¶ä»–å‚æ•°( wte , wpe , blocks , ln_f , n_head )ä¸éœ€è¦æ‰¹å¤„ç†ï¼Œæ‰€æœ‰æ‰¹æ¬¡å…±äº«è¿™äº›å‚æ•°

æ‰¹å¤„ç†åçš„å‡½æ•°å¯ä»¥å¤„ç†å½¢çŠ¶ä¸º `[batch, seq_len]` çš„è¾“å…¥ï¼Œå¹¶è¿”å›å½¢çŠ¶ä¸º `[batch, seq_len, vocab]` çš„è¾“å‡ºï¼Œè¿™ç§æ–¹æ³•çš„ä¼˜ç‚¹æ˜¯æˆ‘ä»¬ä¸éœ€è¦ä¿®æ”¹åŸå§‹çš„ gpt2 å‡½æ•°ï¼ŒJAX ä¼šè‡ªåŠ¨å¤„ç†æ‰¹å¤„ç†é€»è¾‘ã€‚

## æ¨ç†ä¼˜åŒ–(Inference Optimization)

æˆ‘ä»¬çš„å®ç°ç›¸å½“ä½æ•ˆã€‚é™¤äº†GPUæ”¯æŒå’Œæ‰¹å¤„ç†å¤–ï¼Œæœ€å¿«ä¸”å½±å“æœ€å¤§çš„ä¼˜åŒ–æ˜¯å®ç°KVç¼“å­˜(key-value cache)ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç›®å‰æ˜¯é¡ºåºè®¡ç®—æ³¨æ„åŠ›å¤´ï¼Œè€Œå®é™…ä¸Šåº”è¯¥å¹¶è¡Œè®¡ç®—ã€‚

KVç¼“å­˜ï¼šKVç¼“å­˜çš„åŸºæœ¬æ€æƒ³æ˜¯åœ¨è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œä¿å­˜å¹¶é‡ç”¨ä¹‹å‰è®¡ç®—è¿‡çš„é”®(K)å’Œå€¼(V)çŸ©é˜µï¼Œé¿å…é‡å¤è®¡ç®—ã€‚è¿™å¯¹äºé•¿åºåˆ—ç”Ÿæˆç‰¹åˆ«æœ‰æ•ˆï¼Œå› ä¸ºæ¯ç”Ÿæˆä¸€ä¸ªæ–°tokenï¼Œæˆ‘ä»¬åªéœ€è¦è®¡ç®—æ–°tokençš„æŸ¥è¯¢(Q)ã€é”®(K)å’Œå€¼(V)ï¼Œç„¶åå°†æ–°çš„Kå’ŒVæ·»åŠ åˆ°ç¼“å­˜ä¸­ã€‚

æ›´å¤šä¼˜åŒ–èµ„æºï¼šè¿˜æœ‰è®¸å¤šå…¶ä»–æ¨ç†ä¼˜åŒ–æ–¹æ³•ã€‚å¦‚ [Large Transformer Model Inference Optimization](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)ã€[Transformer Inference Arithmetic](Transformer Inference Arithmetic)ï¼Œè¿™äº›èµ„æºè¯¦ç»†ä»‹ç»äº†å„ç§ä¼˜åŒ–æŠ€æœ¯ï¼Œå¦‚é‡åŒ–ã€å‰ªæã€è’¸é¦ã€å¼ é‡å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œç­‰ï¼Œå¯ä»¥æ˜¾è‘—æé«˜å¤§å‹Transformeræ¨¡å‹çš„æ¨ç†æ•ˆç‡ã€‚

## è®­ç»ƒ(Training)

è®­ç»ƒGPTæ¨¡å‹ä¸è®­ç»ƒæ ‡å‡†ç¥ç»ç½‘ç»œç±»ä¼¼(ç›¸å¯¹äºæŸå¤±å‡½æ•°çš„æ¢¯åº¦ä¸‹é™)ã€‚å½“ç„¶ï¼Œåœ¨è®­ç»ƒGPTæ—¶ï¼Œä½ è¿˜éœ€è¦ä½¿ç”¨ä¸€ç³»åˆ—æ ‡å‡†æŠ€å·§(å³ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œæ‰¾åˆ°æœ€ä½³å­¦ä¹ ç‡ï¼Œé€šè¿‡dropoutå’Œ/æˆ–æƒé‡è¡°å‡è¿›è¡Œæ­£åˆ™åŒ–ï¼Œä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œä½¿ç”¨æ­£ç¡®çš„æƒé‡åˆå§‹åŒ–ï¼Œæ‰¹å¤„ç†ç­‰...)ã€‚

è®­ç»ƒä¸€ä¸ªå¥½çš„GPTæ¨¡å‹çš„çœŸæ­£ç§˜è¯€åœ¨äºèƒ½å¤Ÿæ‰©å±•æ•°æ®å’Œæ¨¡å‹ï¼Œè¿™ä¹Ÿæ˜¯çœŸæ­£çš„æŒ‘æˆ˜æ‰€åœ¨ã€‚

å¯¹äºæ•°æ®æ‰©å±•ï¼Œä½ éœ€è¦ä¸€ä¸ªå¤§å‹ã€é«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„æ–‡æœ¬è¯­æ–™åº“ï¼š

1. å¤§å‹ æ„å‘³ç€æ•°åäº¿ä¸ªæ ‡è®°(TBçº§æ•°æ®)ã€‚ä¾‹å¦‚ï¼ŒæŸ¥çœ‹ [The Pile](https://pile.eleuther.ai/)ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€æºé¢„è®­ç»ƒæ•°æ®é›†ã€‚
2. é«˜è´¨é‡ æ„å‘³ç€ä½ éœ€è¦è¿‡æ»¤æ‰é‡å¤çš„ä¾‹å­ã€æ ¼å¼ä¸æ­£ç¡®çš„æ–‡æœ¬ã€ä¸è¿è´¯çš„æ–‡æœ¬ã€åƒåœ¾æ–‡æœ¬ç­‰...
3. å¤šæ ·åŒ– æ„å‘³ç€ä¸åŒé•¿åº¦çš„åºåˆ—ï¼Œæ¶‰åŠè®¸å¤šä¸åŒçš„ä¸»é¢˜ï¼Œæ¥è‡ªä¸åŒçš„æ¥æºï¼Œå…·æœ‰ä¸åŒçš„è§‚ç‚¹ç­‰...å½“ç„¶ï¼Œå¦‚æœæ•°æ®ä¸­å­˜åœ¨ä»»ä½•åè§ï¼Œå®ƒä¹Ÿä¼šåæ˜ åœ¨æ¨¡å‹ä¸­ï¼Œæ‰€ä»¥ä½ ä¹Ÿéœ€è¦æ³¨æ„è¿™ä¸€ç‚¹ã€‚

å°†æ¨¡å‹æ‰©å±•åˆ°æ•°åäº¿å‚æ•°æ¶‰åŠå¤§é‡çš„å·¥ç¨‹å·¥ä½œ(ä»¥åŠèµ„é‡‘ï¼Œå“ˆå“ˆ)ã€‚è®­ç»ƒæ¡†æ¶å¯èƒ½å˜å¾—å¼‚å¸¸å¤æ‚å’Œå†—é•¿ã€‚ä¸€ä¸ªå¥½çš„èµ·ç‚¹æ˜¯ [How to Train Really Large Models on Many GPUs?]( https://lilianweng.github.io/posts/2021-09-25-train-large/)ã€‚åœ¨è¿™ä¸ªä¸»é¢˜ä¸Šï¼Œè¿˜æœ‰ [NVIDIA çš„ Megatron æ¡†æ¶](https://github.com/NVIDIA/Megatron-LM)ã€[Cohere çš„è®­ç»ƒæ¡†æ¶](https://arxiv.org/abs/2204.06514)ã€[Google çš„ PALM](https://huggingface.co/papers/2204.02311)ã€ç”¨äºè®­ç»ƒ EleutherAI å¼€æºæ¨¡å‹çš„å¼€ [mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax)ï¼Œä»¥åŠæ›´å¤šå…¶ä»–æ¡†æ¶ã€‚

## è¯„ä¼°(Evaluation)

å¦‚ä½•è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼Ÿè¿™æ˜¯ä¸€ä¸ªéå¸¸å›°éš¾çš„é—®é¢˜ã€‚[HELM](https://arxiv.org/abs/2211.09110) ç›¸å½“å…¨é¢ï¼Œæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ï¼Œä½†ä½ åº”è¯¥å§‹ç»ˆå¯¹åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æŒ‡æ ‡æŒæ€€ç–‘æ€åº¦ã€‚

## æ¶æ„æ”¹è¿›(Architecture Improvements)

æˆ‘å»ºè®®çœ‹çœ‹ [X-Transformers](https://github.com/lucidrains/x-transformers)ã€‚å®ƒåŒ…å«äº†å…³äº Transformer æ¶æ„çš„æœ€æ–°æœ€å…ˆè¿›çš„ç ”ç©¶ã€‚

## åœæ­¢ç”Ÿæˆ(Stopping Generation)

æˆ‘ä»¬å½“å‰çš„å®ç°è¦æ±‚æˆ‘ä»¬æå‰æŒ‡å®šæƒ³è¦ç”Ÿæˆçš„ç¡®åˆ‡ token æ•°é‡ã€‚è¿™ä¸æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ–¹æ³•ï¼Œå› ä¸ºæˆ‘ä»¬çš„ç”Ÿæˆç»“æœå¯èƒ½å¤ªé•¿ã€å¤ªçŸ­æˆ–åœ¨å¥å­ä¸­é—´è¢«æˆªæ–­ã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥ä¸€ä¸ªç‰¹æ®Šçš„å¥å­ç»“æŸ(EOS)æ ‡è®°ã€‚åœ¨é¢„è®­ç»ƒæœŸé—´ï¼Œæˆ‘ä»¬å°†EOSæ ‡è®°é™„åŠ åˆ°è¾“å…¥çš„æœ«å°¾(ä¾‹å¦‚ï¼Œtokens = ["not", "all", "heroes", "wear", "capes", ".", "<|EOS|>"])ã€‚åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åªéœ€åœ¨é‡åˆ°EOSæ ‡è®°æ—¶åœæ­¢(æˆ–è€…å¦‚æœæˆ‘ä»¬è¾¾åˆ°æŸä¸ªæœ€å¤§åºåˆ—é•¿åº¦)ï¼š

```python
def generate(inputs, eos_id, max_seq_len): 
    prompt_len = len(inputs) 
    while inputs[-1] != eos_id and len(inputs) < max_seq_len: 
        output = gpt(inputs) 
        next_id = np.argmax(output[-1]) 
        inputs.append(int(next_id)) 
    return inputs[prompt_len:] 
```

GPT-2 åœ¨é¢„è®­ç»ƒæ—¶æ²¡æœ‰ä½¿ç”¨EOSæ ‡è®°ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸èƒ½åœ¨æˆ‘ä»¬çš„ä»£ç ä¸­ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œä½†ç°åœ¨å¤§å¤šæ•° LLM éƒ½ä½¿ç”¨ EOS æ ‡è®°ã€‚

## å¾®è°ƒ(Fine-tuning)

æˆ‘ä»¬åœ¨è®­ç»ƒéƒ¨åˆ†ç®€è¦æåˆ°äº†å¾®è°ƒã€‚å›é¡¾ä¸€ä¸‹ï¼Œå¾®è°ƒæ˜¯æŒ‡æˆ‘ä»¬é‡ç”¨é¢„è®­ç»ƒæƒé‡æ¥è®­ç»ƒæ¨¡å‹å®ŒæˆæŸäº›ä¸‹æ¸¸ä»»åŠ¡ã€‚æˆ‘ä»¬ç§°è¿™ä¸ªè¿‡ç¨‹ä¸ºè¿ç§»å­¦ä¹ ã€‚

ç†è®ºä¸Šï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬æç¤ºæ¥è®©æ¨¡å‹å®Œæˆæˆ‘ä»¬çš„ä»»åŠ¡ï¼Œä½†æ˜¯ï¼Œå¦‚æœä½ æœ‰æ ‡è®°æ•°æ®é›†ï¼Œå¾®è°ƒGPTä¼šäº§ç”Ÿæ›´å¥½çš„ç»“æœ(è¿™äº›ç»“æœå¯ä»¥éšç€é¢å¤–æ•°æ®å’Œæ›´é«˜è´¨é‡æ•°æ®çš„å¢åŠ è€Œæ‰©å±•)ã€‚

å¾®è°ƒç›¸å…³çš„ä¸»é¢˜æœ‰å‡ ä¸ªï¼Œæˆ‘å°†å®ƒä»¬åˆ†è§£å¦‚ä¸‹ï¼š

### åˆ†ç±»å¾®è°ƒ(Classification Fine-tuning)

åœ¨åˆ†ç±»å¾®è°ƒä¸­ï¼Œæˆ‘ä»¬ç»™æ¨¡å‹ä¸€äº›æ–‡æœ¬ï¼Œå¹¶è¦æ±‚å®ƒé¢„æµ‹è¯¥æ–‡æœ¬å±äºå“ªä¸ªç±»åˆ«ã€‚ä¾‹å¦‚ï¼Œè€ƒè™‘ IMDB æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å°†ç”µå½±è¯„ä¸ºå¥½æˆ–åçš„ç”µå½±è¯„è®ºï¼š

```
--- ç¤ºä¾‹1 ---
æ–‡æœ¬ï¼šæˆ‘ä¸ä¼šåœ¨ä¸€ç¾å…ƒç§Ÿèµä¹‹å¤œç§Ÿè¿™éƒ¨ç”µå½±ã€‚
æ ‡ç­¾ï¼šå·®
--- ç¤ºä¾‹2 ---
æ–‡æœ¬ï¼šæˆ‘ä¸çŸ¥é“ä¸ºä»€ä¹ˆæˆ‘è¿™ä¹ˆå–œæ¬¢è¿™éƒ¨ç”µå½±ï¼Œä½†æˆ‘ä»ä¸åŒå€¦çœ‹å®ƒã€‚
æ ‡ç­¾ï¼šå¥½
--- ç¤ºä¾‹3 ---
...
```

è¦å¾®è°ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å°†è¯­è¨€å»ºæ¨¡å¤´æ›¿æ¢ä¸ºåˆ†ç±»å¤´ï¼Œå¹¶å°†å…¶åº”ç”¨äºæœ€åä¸€ä¸ª token è¾“å‡ºï¼š

```python
def gpt2(inputs, wte, wpe, blocks, ln_f, cls_head, n_head): 
    x = wte[inputs] + wpe[range(len(inputs))] 
    for block in blocks: 
        x = transformer_block(x, **block, n_head=n_head) 
    x = layer_norm(x, **ln_f) 

    # æŠ•å½±åˆ°n_classes 
    # [n_embd] @ [n_embd, n_classes] -> [n_classes] 
    return x[-1] @ cls_head 
```

æˆ‘ä»¬åªä½¿ç”¨æœ€åä¸€ä¸ªtokenè¾“å‡º x[-1] ï¼Œå› ä¸ºæˆ‘ä»¬åªéœ€è¦ä¸ºæ•´ä¸ªè¾“å…¥ç”Ÿæˆä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œè€Œä¸æ˜¯åƒè¯­è¨€å»ºæ¨¡é‚£æ ·ç”Ÿæˆn_seqä¸ªåˆ†å¸ƒã€‚æˆ‘ä»¬ç‰¹åˆ«é€‰æ‹©æœ€åä¸€ä¸ªtoken(è€Œä¸æ˜¯ç¬¬ä¸€ä¸ªtokenæˆ–æ‰€æœ‰tokençš„ç»„åˆ)ï¼Œå› ä¸ºæœ€åä¸€ä¸ªtokenæ˜¯å”¯ä¸€å…è®¸å…³æ³¨æ•´ä¸ªåºåˆ—çš„tokenï¼Œå› æ­¤å®ƒåŒ…å«æœ‰å…³æ•´ä¸ªè¾“å…¥æ–‡æœ¬çš„ä¿¡æ¯ã€‚

åƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬ç›¸å¯¹äºäº¤å‰ç†µæŸå¤±è¿›è¡Œä¼˜åŒ–ï¼š

```python
def singe_example_loss_fn(inputs: list[int], label: int, params) -> float: 
    logits = gpt(inputs, **params) 
    probs = softmax(logits) 
    loss = -np.log(probs[label]) # äº¤å‰ç†µæŸå¤± 
    return loss 
```

### æŒ‡ä»¤å¾®è°ƒ(Instruction Fine-tuning)

å¦‚ä»Šï¼Œå¤§å¤šæ•°æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢„è®­ç»ƒåè¿˜ä¼šç»å†é¢å¤–çš„æŒ‡ä»¤å¾®è°ƒæ­¥éª¤ã€‚åœ¨è¿™ä¸€æ­¥éª¤ä¸­ï¼Œæ¨¡å‹åœ¨æ•°åƒä¸ªäººå·¥æ ‡è®°çš„æŒ‡ä»¤æç¤º+å®Œæˆå¯¹ä¸Šè¿›è¡Œ(ç”Ÿæˆå¼)å¾®è°ƒã€‚æŒ‡ä»¤å¾®è°ƒä¹Ÿå¯ä»¥ç§°ä¸ºç›‘ç£å¾®è°ƒï¼Œå› ä¸ºæ•°æ®æ˜¯äººå·¥æ ‡è®°çš„(å³ç›‘ç£çš„)ã€‚

é‚£ä¹ˆæŒ‡ä»¤å¾®è°ƒçš„å¥½å¤„æ˜¯ä»€ä¹ˆï¼Ÿè™½ç„¶é¢„æµ‹ç»´åŸºç™¾ç§‘æ–‡ç« ä¸­çš„ä¸‹ä¸€ä¸ªè¯ä½¿æ¨¡å‹æ“…é•¿ç»§ç»­å¥å­ï¼Œä½†è¿™å¹¶ä¸ä½¿å®ƒç‰¹åˆ«æ“…é•¿éµå¾ªæŒ‡ä»¤ã€è¿›è¡Œå¯¹è¯æˆ–æ€»ç»“æ–‡æ¡£(è¿™äº›éƒ½æ˜¯æˆ‘ä»¬å¸Œæœ›GPTåšçš„äº‹æƒ…)ã€‚åœ¨äººå·¥æ ‡è®°çš„æŒ‡ä»¤+å®Œæˆå¯¹ä¸Šå¾®è°ƒå®ƒä»¬æ˜¯æ•™å¯¼æ¨¡å‹å¦‚ä½•å˜å¾—æ›´æœ‰ç”¨çš„æ–¹å¼ï¼Œå¹¶ä½¿å®ƒä»¬æ›´å®¹æ˜“äº¤äº’ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºAIå¯¹é½ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨ä½¿æ¨¡å‹æŒ‰ç…§æˆ‘ä»¬å¸Œæœ›çš„æ–¹å¼è¡Œäº‹å’Œè¡¨ç°ã€‚å¯¹é½æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸï¼Œä¸ä»…åŒ…æ‹¬éµå¾ªæŒ‡ä»¤(è¿˜åŒ…æ‹¬åè§ã€å®‰å…¨æ€§ã€æ„å›¾ç­‰)ã€‚

è¿™äº›æŒ‡ä»¤æ•°æ®å…·ä½“æ˜¯ä»€ä¹ˆæ ·å­ï¼ŸGoogleçš„ [FLAN æ¨¡å‹](https://huggingface.co/papers/2109.01652)æ˜¯åœ¨å„ç§å­¦æœ¯NLPæ•°æ®é›†(å·²ç»æ˜¯äººå·¥æ ‡è®°çš„)ä¸Šè®­ç»ƒçš„ï¼š

![Figure 3: Datasets and task clusters used in this paper (../../README.assets/datasets_and_task_clusters_used_in_this_paper.png).](./README.assets/datasets_and_task_clusters_used_in_this_paper.png)

å¦ä¸€æ–¹é¢ï¼ŒOpenAIçš„ [InstructGPT](https://huggingface.co/papers/2203.02155) æ˜¯åœ¨ä»ä»–ä»¬è‡ªå·±çš„APIæ”¶é›†çš„æç¤ºä¸Šè®­ç»ƒçš„ã€‚ç„¶åä»–ä»¬ä»˜è´¹è®©å·¥ä½œäººå‘˜ä¸ºè¿™äº›æç¤ºç¼–å†™å®Œæˆå†…å®¹ã€‚ä»¥ä¸‹æ˜¯æ•°æ®çš„ç»†åˆ†ï¼š

![Table 1 and 2 from InstructGPT paper](./README.assets/table_1_and_2_from_instructgpt_paper.png)

### å‚æ•°é«˜æ•ˆå¾®è°ƒ(Parameter Efficient Fine-tuning)

å½“æˆ‘ä»¬åœ¨ä¸Šè¿°éƒ¨åˆ†è°ˆè®ºå¾®è°ƒæ—¶ï¼Œå‡è®¾æˆ‘ä»¬æ­£åœ¨æ›´æ–°æ‰€æœ‰æ¨¡å‹å‚æ•°ã€‚è™½ç„¶è¿™ä¼šäº§ç”Ÿæœ€ä½³æ€§èƒ½ï¼Œä½†åœ¨è®¡ç®—æ–¹é¢(éœ€è¦åœ¨æ•´ä¸ªæ¨¡å‹ä¸Šåå‘ä¼ æ’­)å’Œå­˜å‚¨æ–¹é¢(å¯¹äºæ¯ä¸ªå¾®è°ƒæ¨¡å‹ï¼Œä½ éœ€è¦å­˜å‚¨å‚æ•°çš„å…¨æ–°å‰¯æœ¬)éƒ½å¾ˆæ˜‚è´µã€‚å¯¹äºæŒ‡ä»¤å¾®è°ƒï¼Œè¿™æ˜¯å¯ä»¥çš„ï¼Œæˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–æ€§èƒ½ï¼Œä½†å¦‚æœä½ éšåæƒ³ä¸ºå„ç§ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒ100ä¸ªä¸åŒçš„æ¨¡å‹ï¼Œé‚£ä¹ˆä½ å°±ä¼šé‡åˆ°é—®é¢˜ã€‚

è§£å†³è¿™ä¸ªé—®é¢˜çš„æœ€ç®€å•æ–¹æ³•æ˜¯åªæ›´æ–°å¤´éƒ¨å¹¶å†»ç»“(å³ä½¿ä¸å¯è®­ç»ƒ)æ¨¡å‹çš„å…¶ä½™éƒ¨åˆ†ã€‚è¿™å°†åŠ é€Ÿè®­ç»ƒå¹¶å¤§å¤§å‡å°‘æ–°å‚æ•°çš„æ•°é‡ï¼Œä½†å…¶æ€§èƒ½ä¸ä¼šåƒå®Œå…¨å¾®è°ƒé‚£æ ·å¥½(æˆ‘ä»¬ç¼ºä¹æ·±åº¦å­¦ä¹ ä¸­çš„"æ·±åº¦")ã€‚æˆ‘ä»¬å¯ä»¥é€‰æ‹©æ€§åœ°å†»ç»“ç‰¹å®šå±‚(å³å†»ç»“é™¤æœ€å4å±‚ä¹‹å¤–çš„æ‰€æœ‰å±‚ï¼Œæˆ–å†»ç»“æ¯éš”ä¸€å±‚ï¼Œæˆ–å†»ç»“é™¤å¤šå¤´æ³¨æ„åŠ›å‚æ•°ä¹‹å¤–çš„æ‰€æœ‰å‚æ•°)ï¼Œè¿™å°†æœ‰åŠ©äºæ¢å¤ä¸€äº›æ·±åº¦ã€‚è¿™å°†è¡¨ç°å¾—æ›´å¥½ï¼Œä½†æˆ‘ä»¬å˜å¾—ä¸é‚£ä¹ˆå‚æ•°é«˜æ•ˆï¼Œå¹¶å‡å°‘äº†æˆ‘ä»¬çš„è®­ç»ƒé€Ÿåº¦æå‡ã€‚

ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)æ–¹æ³•ã€‚PEFTæ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸï¼Œæœ‰å¾ˆå¤šä¸åŒçš„æ–¹æ³•å¯ä¾›é€‰æ‹©ã€‚

ä¾‹å¦‚ï¼Œä»¥ [Adapters è®ºæ–‡](https://huggingface.co/papers/1902.00751)ä¸ºä¾‹ã€‚åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬åœ¨transformerå—ä¸­çš„FFNå’ŒMHAå±‚ä¹‹åæ·»åŠ ä¸€ä¸ªé¢å¤–çš„"é€‚é…å™¨"å±‚ã€‚é€‚é…å™¨å±‚åªæ˜¯ä¸€ä¸ªç®€å•çš„2å±‚å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œå…¶ä¸­è¾“å…¥å’Œè¾“å‡ºç»´åº¦æ˜¯n_embdï¼Œè€Œéšè—ç»´åº¦å°äºn_embdï¼š

![Figure 2. Architecture of the adapter module and its integration with the Transformer.](../../../../../.././README.assets/architecture_of_the_adapter_module_and_its_integration_with_the_transformer.png)

éšè—ç»´åº¦çš„å¤§å°æ˜¯æˆ‘ä»¬å¯ä»¥è®¾ç½®çš„è¶…å‚æ•°ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨å‚æ•°å’Œæ€§èƒ½ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚å¯¹äºBERTæ¨¡å‹ï¼Œè®ºæ–‡æ˜¾ç¤ºä½¿ç”¨è¿™ç§æ–¹æ³•å¯ä»¥å°†è®­ç»ƒå‚æ•°å‡å°‘åˆ°2%ï¼ŒåŒæ—¶ä¸å®Œå…¨å¾®è°ƒç›¸æ¯”åªæ‰¿å—å¾ˆå°çš„æ€§èƒ½æŸå¤±(<1%)ã€‚
